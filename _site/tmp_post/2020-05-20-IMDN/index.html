<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>IMDN paper review - Hello, world! I'm David Freeman</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Hello, world! I'm David Freeman" property="og:site_name">
  
    <meta content="IMDN paper review" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
" property="og:description">
  
  
    <meta content="http://localhost:4000/tmp_post/2020-05-20-IMDN/" property="og:url">
  
  
    <meta content="2020-05-21T19:32:20+09:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/IMDN.png" property="og:image">
  
  
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="IMDN paper review">
  
  
    <meta name="twitter:url" content="http://localhost:4000/tmp_post/2020-05-20-IMDN/">
  
  
    <meta name="twitter:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/IMDN.png">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/Kim.jpg" alt="Kim Dae-han"></a>
      </div>
      <div class="author-name">Kim Dae-han</div>
      <p>I'm studying/researching about artificial intelligence..</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/koreadhkim" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a></li>
        
        
          <li class="email"><a href="mailto:eogks1525@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2020 &copy; Kim Dae-han</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/IMDN.png alt="IMDN paper review">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">IMDN paper review</h1>
        <div class="page-date"><span>2020, May 21&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1909.11856.pdf (ACM MM 2019)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>논문에서 저자는 과도한 convolution의 사용은 low computing power device에서 사용에 제한이 있다는 말을 꺼내고 있습니다. 또한, arbitrary scale factor 는 previous approaches에서는 해결되지 않았던 practical application에 적용하기 위해 중요한 issue라고 합니다. 위와 같은 문제를 해결하기 위하여, lightweight information multi-distillation network(IMDN) 을 제안합니다. IMDN은 information multi distillation blocks(IMDB)로 구성되어 있으며, 이를 cascade 구조로 construction한다고 합니다. IMDB 에는 distillation/ selective fusion part가 있다고 합니다.
<br /></p>

<p><strong>distillation module : extracts hierarchical features step-by-step</strong><br />
<strong>fusion module : aggregates them according to the importance of candidate features</strong><br /></p>

<p>이 두가지 모듈은 proposed contrast-aware channel attention mechanism 으로 evalution된다고 합니다.<br /></p>

<p>한편, real image(any sizes)를 처리하기 위해서, block-wise-image patches를 super-resolve 하는 Adaptive cropping strategy(ACS)를 develop 하였다고 합니다. <br /></p>

<p>논문의 초록을 보면 이번 논문에서 무슨 문제를 어떻게 해결하겠는지가 어느정도 보입니다. 이번 논문에서는 low-computing power device에서도 사용가능한 shallow 하면서도 성능이 좋은 network 구조를 제안할 것으로 보이고, practical application에 적용하기 위한 어떤 방법을 제안할 것으로 보입니다.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>다수의 SR논문에서 (eg.SISR) Introduction에서 말하는 내용은 비슷합니다.(제가 읽은 SISR에 한해서)<br />
(1) LR –&gt; HR Task이다.<br />
(2) ill-posed problem이다.<br />
(3) 이러한 issue 를 해결하기 위해 많은 방법이 제안되었다.<br />
(4) (3)에서 말한 많은 방법들에 대한 맛보기.<br />
(5) (4)에서 말한 맛보기에서 issue들을 나열<br /></p>

<p>저자는 DRRN, MemNet을 예시로 들어, 이들을 분석한 결과를 제시하고 있습니다. 아래의 그림은 차례로 DRRN, MemNet입니다.<br />
<img src="../assets/img/IMDN/IMDN_01.png" alt="Alt text" /><br />
<img src="../assets/img/IMDN/IMDN_02.png" alt="Alt text" /><br /></p>

<p>저자는, DRRN, MemNet의 Train/Test time이 너무 오래걸리는 것과, network가 GPU memory를 많이 소모하는것에 집중합니다. 저자의 분석은 DRRN, MemNet이 느리고, GPU memory를 많이 사용하는 이유는 input자체가 LR(interpolated) 이며, network 내부에서 downsampling operation을 하지 않아서 라고 합니다. DBPN에 정리가 잘 되어있는데, LR(interpolated)를 선택하여 model을 설계하는것은 계산적인 부분에서 매우 단점으로 작용하게 됩니다. HR space에서 SR이 되기 때문입니다. 그래서 해당 문제를 해결하기 위하여, LR input을 그대로 network에 넣고, deconv, pixel-shuffle 등을 이용하여 network 끝 혹은 중간에, Upsampling 하는 방법들이 제시가 되었습니다. Introduction에서 아주 잘 설명된 부분이 있는데, 바로 아래와 같습니다.<br /></p>

<p><img src="../assets/img/IMDN/IMDN_03.png" alt="Alt text" /><br /></p>

<p>위의 화살표를 따라가면, SRResNet 이후로 SR에서 ResNet-block을 어떻게 사용하여 새로운 논문들이 제안되었는지를 파악할 수 있습니다. 모두 SR doamin에서는 유명한 논문들이기 때문에 관심있으시면 보시면 될 것 같습니다. <br />
위와 같은(논문에는 더 많은 기존의 연구들이 포함되어있음.) 논문에서 제안하는 모델들이 PSNR, SSIM을 개선하기 위해서 더 많은 Layer를 쌓게 되면서 parameter가 커지고, memory 소모량이 커지며, test 속도가 느려지는 결과를 초래하게 되었다고 합니다. 당연히 이러한 network들은 resource 에 제한이 있는 경우 사용할 수 없습니다.<br /></p>

<p>결과적으로, lightweight and efficient model(network)를 제안하는 것이 중요하다고 합니다.<br /></p>

<p>당연히, 실제 사용할 수 있도록 좀더 가볍고, 효율적인 model을 제안하는 연구들도 있었습니다. 논문에서는 여러가지 논문에 대한 이야기를 하고 있습니다. 제안된 network에 대한 분석은 아래와 같습니다.<br /></p>

<h4 id="parameter-감소와-관련된-연구">parameter 감소와 관련된 연구</h4>
<p><strong>[1]:</strong>  DRCN, DRRN, MemNet 은 model 의 크기는 줄었지만, recursive 구조를 사용하면서 발생하는 성능 저하를 depth, width를 늘림으로써 계산량이 많아짐.__따라서, recursive 구조는 사용하면 안된다.<br />
<strong>[2]:</strong>  CANN-M, cascade 구조를 통한 mobile 에서 사용가능한 network를 제안하였지만, PSNR등 성능이 떨어짐. <br />
<strong>[3]:</strong>  IDN(information distillation network)를 제안하였고, 적당한 크기로 좋은 성능을 얻었지만, 개선점이 보인다. <br /></p>

<p>이외에 저자는 network depth 가 속도에 많은 영향을 준다.라고 말하고 있고 이는 당연히 맞는 이야기 입니다.<br />
parameter 감소와 관련된 연구 [3]에서 감이 오셨겠지만, 본 논문에서 IDN 연구의 영감을 받아 information multi-distillation block(IMDB)를 정교하게 설계한다고 합니다.<br /></p>

<p><strong><u>The contributions of this paper can be summarized as follows:</u></strong><br /><br />
<strong><u>[1]</u></strong> : IMDB를 제안합니다. contrast-aware attention(CCA) layer 의 이점으로 인하여 IMDB는 적은 parameter를 사용하여 유의미한 성능을 얻는다고 합니다.<br />
<strong><u>[2]</u></strong> : adaptive cropping strategy(ACS), 임의의 크기의 image를 처리할 수 있다고 합니다. 이를 통해 계산속도와 메모리 사용량을 줄입니다.<br />
<strong><u>[3]</u></strong> : network의 depth가 inferenct time에 영향이 있음을 실험을 통하여 증명합니다.(It can be a guideline for guiding a lightweight network design)<br /></p>

<h2 id="2-related-work">2. Related Work</h2>

<h3 id="-attention-model-">[ Attention model ]</h3>
<h4 id="-senet-">&lt; SENet &gt;</h4>
<p>아래의 그림은 SENet에서 제안한 SE block 입니다.</p>

<p><img src="../assets/img/IMDN/IMDN_04.png" alt="Alt text" /><br />
[1] : input feature map -&gt; GAP –&gt; 1x1xC <br />
[2] : 1x1xC –&gt; Excitation(simple self-gating 형태) <br />
[3] : C(channel)수가 동일한 feature map 에 곱하여, Attention-Mechansim 형성 <br /></p>

<h4 id="-cbam-">&lt; CBAM &gt;</h4>
<p>아래의 그림은 CBAM 에서 제안한 channel, spatial Attention module 입니다. 저자는 CBAM이 SE block을 수정하였다고 말하고 있습니다.
<img src="../assets/img/IMDN/IMDN_05.png" alt="Alt text" /><br /></p>

<h4 id="-non-local-neural-networks-">&lt; Non-local Neural Networks &gt;</h4>
<p>아래의 그림은 Non-local Neural Networks 에서 제안한 것입니다.
<img src="../assets/img/IMDN/IMDN_06.png" alt="Alt text" /><br /></p>

<h2 id="3-method">3. Method</h2>
<p>아래의 그림은 논문에서 제안하는 Network 의 구조입니다. 논문에서 제안하는 방법론에 대한 output은 바로아래의 수식과 같습니다.
<img src="../assets/img/IMDN/IMDN_07.png" alt="Alt text" /><br /></p>

<p><img src="../assets/img/IMDN/IMDN_08.png" alt="Alt text" /><br /></p>

<h3 id="-mae-loss-">[ MAE Loss ]</h3>

<p>지금까지 제가 읽었던 논문과는 다르게, 본 논문에서는 MAE Loss 를 사용하여 network 를 optimize 했다고 합니다. 아래의 그림을 보시면 MAE 는 Outliear 의 영향을 잘 받지 않습니다. 때문에 outliear 에서 robust하다. 라고 볼 수 있습니다. 제 생각에는 MAE 를 SR에서 사용하는 것은 Dataset에 overfiting 될 수 있다고 볼 수 있지 않을까? 라는 생각을 해보게 됩니다. 예를 들어 같은 network를 DIV2K를 이용하여, MSE Loss, MAE Loss를 사용하여 train 하면 DIV2K에서는 MAE Loss가 성능 더 높게 나오겠지만, 다른 image에서는 어쩌면 MSE가 성능이 더 좋지 않을까? 라는 것과 같습니다. 그러나, DIV2K의 경우 많은 경우를 포함하기 때문에, 무조건 MAE가 더 않 좋을 것이라는 것은 아닙니다. DIV2K에서 train 되지않은 어떤 image 저하의 경우 MSE-Loss 를 사용한 network가 더 좋을수도 있을 것 같다라는 겁니다. 어디까지나 저의 생각입니다.
<img src="/assets/img/IMDN/IMDN_10.png" width="40%" height="50%" />
<img src="/assets/img/IMDN/IMDN_11.png" width="40%" height="50%" /></p>

<p><img src="../assets/img/IMDN/IMDN_09.png" alt="Alt text" /><br /></p>

<h3 id="-information-multi-distillation-block-imdb-">[ Information multi-distillation block (IMDB) ]</h3>

<p><img src="../assets/img/IMDN/IMDN_12.png" alt="Alt text" />
<img src="/assets/img/IMDN/IMDN_13.png" width="40%" height="50%" /></p>

<p>위의 그림이 논문에서 제안하는 IMDB 입니다. 우측 상단의 그림은 좌측 그림의 PRM의 세부정보를 나타냅니다. 아래의 수식은 위의 IMDB의 흐름입니다.<br /></p>

<p><img src="../assets/img/IMDN/IMDN_14.png" alt="Alt text" />
<img src="../assets/img/IMDN/IMDN_15.png" alt="Alt text" /></p>

<h3 id="-contrast-aware-channel-attention-layer-">[ Contrast-aware channel attention layer ]</h3>

<p>앞서, 말씀드린 SEmodule은 channel attention으로 잘 알려져 있다고 합니다. 저자는 다음과 같이 말합니다.<br /></p>

<blockquote>In the high-level field, the importance of a feature map depends on activated high-value areas, since these regions in favor of classification or detection. Accordingly, global average/maximum pooling is utilized to capture the global information in these high-level or mid-level vision. </blockquote>

<p><img src="../assets/img/IMDN/IMDN_16.png" alt="Alt text" /><br /></p>

<p>위의 그림이 논문에서 제안하는 CCA입니다. 저자는 GAP(global average pooling)은 PSNR은 높아질 수 있을지라도, SSIM과 같이 image의 detail(structures, textures, edges)와 관련된 정보를 처리하기에는 부족하다고 합니다. 따라서, SR과 같은 low-level vision task에서 적절한 CCA를 제안하는 것입니다. GAP를 standard deviation / mean 으로 대체한다고 합니다. 이는 feture map 의 contrast를 대조하기 위함이라고 합니다. 아래의 수식을 보시면 될 것 같습니다.<br /></p>

<p><img src="../assets/img/IMDN/IMDN_17.png" alt="Alt text" /><br />
아래는 논문에서 설명하는 위 수식에 대한 정보 입니다.</p>
<blockquote> Let’s denote X = [x<sub>1</sub>, . . . , x<sub>c</sub> , . . . , x<sub>C</sub>] as the input, which has C feature maps with spatial size of H ×W. where z<sub>c</sub> is the c-th element of output. H<sub>GC</sub> (·) indicates the global contrast (GC) information evaluation function. With the assistance of the CCA module, our network can steadily improve the accuracy of SISR. </blockquote>

<h3 id="-adaptive-cropping-strategy-">[ Adaptive cropping strategy ]</h3>
<p>저자는 아래의 그림과 같은 ACS 를 통하여 single model 에서 multi-scale factor 에 대한 문제도 해결하였다고 합니다. 위에서 본 IMDN의 구조와는 다른 것이 눈에 보이실 겁니다. 아래와 같은 network를 IMDN_AS라고 합니다. 이는 기존의 IMDN에서 두개의 downsampling layer를 추가하였다고 합니다.<br />
<img src="../assets/img/IMDN/IMDN_18.png" alt="Alt text" /><br /></p>

<p>아래의 그림은 어떻게 해서 any-scale factor에 대한 문제를 해결했는지에 관한것 입니다. 우선, 해당하는 방법은 LR, HR size가 동일합니다.<br />
즉, 어떤 image 가 input으로 들어오더라도 동인한 size로 SR을 수행하겠다. 그에 대한 방법론이다. 라고 하는 것으로 파악됩니다. 원리는 다음과 같습니다.</p>

<p><img src="../assets/img/IMDN/IMDN_19.png" alt="Alt text" /><br /></p>

<p>아래는 위 그림에 대한 설명입니다.</p>

<p><img src="../assets/img/IMDN/IMDN_20.png" alt="Alt text" /><br /></p>

<h3 id="-experiment-preveiw-">[ Experiment preveiw ]</h3>

<p>training dataset = DIV2K<br />
benchmark = Set5, Set14, B100, Urban100, Manga109<br />
이전 network와 동등비교 하기 위하여 Y(luminance) channel 에서 만 evaluation 합니다. <br /></p>

<p>Training settings : 90,180,270 rotation, horizontal flip<br />
patch size : 192x192<br />
batch size : 16<br />
IMDB : 6개 사용<br />
optimizer : Adam<br />
hardware : NVIDIA Titan Xp<br /></p>

<h3 id="-experiment-ablation-studies-of-cca-module-and-iic-scheme-">[ Experiment Ablation studies of CCA module and IIC scheme ]</h3>
<p><img src="../assets/img/IMDN/IMDN_21.png" alt="Alt text" /><br /></p>

<p>CCA,IIC 방식의 효과를 검증하기 위한 실험이라고 합니다. IMDB를 4개 사용하여 위와 같은 IMDN_basic_B4를 설정하여 실험하였습니다.</p>

<p><img src="../assets/img/IMDN/IMDN_22.png" alt="Alt text" /><br />
위의 표를 보면, 2K parameter증가(CCA 적용)으로 인하여 성능이 향상되는 것을 확인 하였다고 합니다.</p>

<p><img src="../assets/img/IMDN/IMDN_23.png" alt="Alt text" /><br />
IMDN_basic_B4는 PRM 만을 포함합니다. 위의 표는 CCA의 성능이 CA 보다 좋은 것을 확인하기 위하여 실험한 것이라고 합니다. 결과적으로, CA 보다 CCA가 조금 더 성능이 좋습니다.</p>

<h3 id="-experiment-imdbprm-">[ Experiment IMDB(PRM) ]</h3>
<p><img src="../assets/img/IMDN/IMDN_24.png" alt="Alt text" /><br />
아래와 같이 수정하여 실험하였고, 결과적으로, 510K 로 parameter 수가 많음에도 불구하고 성능이 더떨어지는 것을 확인할 수 있습니다. 즉, PRM이 성능향상에 중요한 역할을 한다. 라고 볼 수 있을 것 같습니다.</p>

<blockquote> To study the efficiency of PRM in IMDB, we replace it with three cascaded 3 × 3 convolution layers (64 channels) and remove the final 1 × 1 convolution (used for fusion).</blockquote>

<h3 id="-experiment-investigation-of-acs-">[ Experiment Investigation of ACS ]</h3>

<p><img src="../assets/img/IMDN/IMDN_25.png" alt="Alt text" /><br /></p>

<p>Dataset은 RealSR dataset from NTIRE2019 Real Super-Resolution Challenge를 사용하였으며, train(60),val(20)으로 구성되어 있습니다. LR,HR 은 같은 size를 갖고 있다고 합니다.
결과적으로, 위의 표와 같이 RealSR에서도 좋은 성능향상을 보이고 있습니다.</p>

<h3 id="-experiment-running-timedepth-parameter-">[ Experiment Running Time(depth, parameter) ]</h3>
<p><img src="../assets/img/IMDN/IMDN_26.png" alt="Alt text" /><br /></p>

<p>위의 표에서 EDSR, RCAN을 보시면, EDSR은 RCAN에 비해 훨씬 많으 parameter를 보유하고 있지만, RCAN보다 빠른 inference time 을 보여주고 있습니다. 당연히 parameter가 많으므로 memory는 많이 사용합니다.
결과적으로, 위의 표에서 말하고자 하는 것은 depth가 inference time에 많은 영향을 준다는 것입니다. EDSR은 parameter가 많지만 병렬로 계산되기 때문에 빠르다고 말하고 있습니다.</p>

<h3 id="-experiment-benchmark-">[ Experiment benchmark ]</h3>
<p><img src="../assets/img/IMDN/IMDN_27.png" alt="Alt text" /><br /></p>

<h3 id="-result-trade-off-between-performance-and-running-time-on-set5-4-dataset-">[ Result Trade-off between performance and running time on Set5 ×4 dataset ]</h3>

<p><img src="../assets/img/IMDN/IMDN_28.png" alt="Alt text" /><br /></p>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=IMDN paper review&url=http://localhost:4000/tmp_post/2020-05-20-IMDN/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/tmp_post/2020-05-20-IMDN/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/tmp_post/2020-05-20-IMDN/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
    <div id="disqus_thread" class="article-comments"></div>
    <script>
      (function() {
          var d = document, s = d.createElement('script');
          s.src = '//mr-brown.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
