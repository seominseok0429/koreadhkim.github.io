I")<h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1808.08718.pdf (CVPR 2018, NTIRE18 Champion)</p>

<hr style="border:solid 1px green;" />

<h2 id="0-abstract">0. Abstract</h2>

<h3 id="1-논문에서-same-parameter--computational-budget으로-더-나은-성능을-가지는-model-을-제안하겠다">[1] 논문에서 same parameter / computational budget으로 더 나은 성능을 가지는 Model 을 제안하겠다.</h3>
<h3 id="2-1을-위한-방법으로-residual-network-에서-activation-function-전에-activation-map을-늘린다-x2-x4-이는-slim-identity-pathway를-갖는다">[2] [1]을 위한 방법으로 residual network 에서 activation function 전에 activation map을 늘린다. (x2, x4) 이는 slim identity pathway를 갖는다.</h3>
<h3 id="3-2에서-더-나아가-x6-x9로-확대하기-위해서-linear-low-rank-convolution을-삽입하여-accuracy-efficiency-tradeoff를-해결한다">[3] [2]에서 더 나아가 (x6, x9)로 확대하기 위해서 linear low-rank convolution을 삽입하여 accuracy-efficiency tradeoff를 해결한다.</h3>
<h3 id="4-bn-or-no-bn을-비교한다-weight-normalization이-더-좋은-성능을-보이는-것을-확인한다">[4] BN or no BN을 비교한다. weight normalization이 더 좋은 성능을 보이는 것을 확인한다.</h3>

<hr style="border:solid 1px green;" />

<h2 id="1-introduction">1. Introduction</h2>
<p>다수의 SR논문에서 (eg.SISR) Introduction에서 말하는 내용은 비슷합니다.(제가 읽은 SISR에 한해서)<br />
(1) LR –&gt; HR Task이다.<br />
(2) ill-posed problem이다.<br />
(3) 이러한 issue 를 해결하기 위해 많은 방법이 제안되었다.<br />
(4) (3)에서 말한 많은 방법들에 대한 맛보기.<br />
(5) (4)에서 말한 맛보기에서 issue들을 나열<br /></p>

<hr />

<p><strong>[1] :</strong> SRCNN, FSRCNN, ESPCN 은 상대적으로 적은 layer를 사용하여 SR task에 적용하였다.<br />
<strong>[2] :</strong> VDSR, SRResNet, EDSR 은 depth가 크지만, low-level feature를 적절히 활용하지 못하였다.<br />
<strong>[3] :</strong> 해당 문제를 해결하기 위하여, SRDenseNet, RDN, MemNet을 포함한 여러가지 방법들이 제안이 되었다.(low-level/high-level layer 사이의 skip-connection) [3]의 방법론들은 SR을 위한 전체적인 구조를 공식화하는데 기여하였다.<br /></p>

<hr />

<p><strong>[4] :</strong> 해당 논문에서는 [1]~[3]에 대한 것과는 다른 관점을 다룬다. 다양한, shortcut-connection을 추가하면서, ReLU도 함께 추가하게 되는데, 이는 shallow layer –&gt; deeper layer의 information flow 를 방해한다고 추측합니다.<br />
<strong>[5] :</strong> 해당 논문에서는 다른 SR network를 기반으로 하여, 추가적인 parameter/computation 없이 SRDenseNet/MemNet을 포함한 복잡한 skip-connection을 갖는 network들을 모두 제치고 SISR에서 좋은 성능을 달성한다고 합니다.<br />
<strong>[6] :</strong> 해당 논문에서는 직관을 말해주고 있습니다. 논문에서의 직관은 ReLU 이전에 Activation map을 확장하면 더 많은 information이 전달될 수 있는 반면에 성능은 더 높게 유지된다는 것입니다. low-level layer의 low-level feature map 이 더 쉽게 propagation되고 이는 final layer가 더 좋은 성능을 갖고 dense-pixel을 잘 prediction하게 할 수 있다고 합니다.<br />
<strong>[7] :</strong> 또한, wide activation을 적용함에 있어서, ReLU이전에 feature를 확장하기 위한 효율적인 방법이 무엇이 있을까? 를 생각하게 만든다. 따라서, WDSR -A/ WDSR -B를 생각하게 되었다.<br />
<strong>[8] :</strong> WDSR -A는 각 residual block에서 activation되기 이전에 x2~x4의 activation map을 갖는다. 그러나, ReLU이전에 activation map 을 x2,x4로 늘리기 위해서는 어딘가에서 parameter의 감소가 이루어 져야하고, 이를 달성하기 위하여 identity mapping을 더 적게 가져가게 된다. 하지만, x4같은 경우 identitiy mapping이 너무 slim해 지고, 결과적으로 성능은 떨어지게 된다. 따라서, 두번째 방법으로 identitiy mapping path의 channel을 일정하게 유지하고 이를 효율적으로 하기 위하여, group-convolution / depth-wise-separable convolution을 고려하였고, 두 경우 모두 좋지 않았고, 따라서, 초록에서 말했던 linear low-rank convolution을 제안한다.<br />
<strong>[9] :</strong> [8]에서 설명한, WDSR -A의 단점을 보완한, WDSR -B를 설계하고 제안한다. 이는 추가적인 parameter/computation 없이 (wide activation(x6, x9))를 수행할 수 있으며, 성능은 더욱 향상한다.
이는 제한된 상황에서 WDSR에서 의 방법이 일관되게 baseline 을 초과하는 성능을 보일 수 있다고 말하고 있습니다.
<strong>[10] :</strong> 추가적으로 논문에서는 BN / No BN에 대한 비교를 직접적으로 실험하게 됩니다. 이는 이전의 논문에서 모두 얘기했던 내용이었는데(BN 은 SR task에 적합하지 않다.) 이유로는 1) mini_batch에 의존성. 2)train/test에서 다르게 공식화 되는 문제. 3) 강한 normalization의 부작용. 과 같은 3가지의 직관에 관련된 실험을 한다고 합니다. 또한, weight normalization이 더 좋은 성능을 보일 수 있다는 것을 증명하게 됩니다.</p>

<p><strong><u>In summary, our contributions are as follows.:</u></strong><br /></p>
<h4 id="1--sisr-에서의-residual-network에서-wide-activation-이-더-좋은-성능을-갖는다는-것을-증명한다">[1] : SISR 에서의 residual network에서 wide activation 이 더 좋은 성능을 갖는다는 것을 증명한다.</h4>
<h4 id="2--wide-activationwdsr--ax2-x4를-제안하며-효율적으로-설계하기-위한-wdsr--bx6-x9를-제안한다">[2] : wide activation(WDSR -A(x2, x4))를 제안하며, 효율적으로 설계하기 위한 (WDSR -B(x6, x9))를 제안한다.</h4>
<h4 id="3--bn은-sr-task에-적합하지-않다는-것을-확인하고-sr-task에서-weight-normalization은-성능을-향상시키기-때문에-도입하는-것을-제안한다">[3] : BN은 SR task에 적합하지 않다는 것을 확인하고, SR Task에서 weight normalization은 성능을 향상시키기 때문에, 도입하는 것을 제안한다.</h4>
<h4 id="4--weight-normalization-과-함께-wdsr--a-wdsr--b의-성능을-확인한다">[4] : weight normalization 과 함께, WDSR -A, WDSR -B의 성능을 확인한다.</h4>
<hr style="border:solid 1px green;" />

<h2 id="2-related-work">2. Related Work</h2>

<h2 id="-super-resolution-network-">[ Super-Resolution Network ]</h2>

<h3 id="-upsampling-layers-">[ Upsampling layers ]</h3>
<p>input space 에서는 DBPN에서 기존의 네트워크들의 SR task 접근 방식을 input space 어떻게 하는지 보시면 될 것 같습니다. ESPCN에서 사용한 pixel- shuffle 을 통한 네트워크 구조를 제안하였는데, 이는 checkerboard artifact가 현저하게 줄어듬을 보여주었다고 합니다.</p>

<h3 id="-very-deep-and-recursive-neural-networks-">[ Very deep and recursive neural networks ]</h3>
<p>Deep, recursive 두 단어로도, 바로 연상되는 네트워크들이 있을텐데요, VDSR,RDN… 뭐 결과적으로 depth가 깊어지면 training이 어렵고, 이를 해결하기 위해서 recursive method를 도입한 네트워크가 제안되었다. 라는 이야기 입니다.</p>

<h3 id="-skip-connections-">[ Skip connections ]</h3>
<p>low-level feature 는 SR task에서 중요하다고 합니다. 따라서, skip-connection이 적용된 네트워크(VDSR, SRResNet, SRDenseNet)등이 SISR에서 깊은 네트워크에 대한 이해를 하고 설계한 것으로 보고 있고, MemNet은 skip-connection + recursive 하였고, RDN 은 residual dense network 제안하였습니다. 결과적으로 설명된 모든 network는 skip connection 을 통하여 좋은 성능을 보였습니다.</p>

<h3 id="-normalization-layers-">[ Normalization layers ]</h3>
<p>해당 section에서는 deep network가 training이 힘들기 때문에, training 을 단순화 시킬 수 있는 방법중에 하나로 BN을 이야기 하고 있지만, 결과적으로, BN layer가 SR task에서는 성능을 저하시키기 때문에 사용하지 않는 것을 이야기 하고 있다.</p>

<h2 id="-parameter-efficient-convolutions-">[ Parameter-Efficient Convolutions ]</h2>

<h3 id="-flattened-convolution-">[ Flattened convolution ]</h3>

<p>Flattened convolution에서 parameter 수는 XYC –&gt; X+Y+C로 감소하게 된다. 여기서, C = feature map , X/Y = Width, height 이다. 아래의 그림은 flattened convolution 논문의 그림입니다.</p>

<p><img src="../assets/img/WDSR/WDSR_01.png" alt="Alt text" /><br /></p>

<h3 id="-group-convolution-">[ Group convolution ]</h3>

<p>ResNext 에서 사용하는 group convolution입니다. 이는 feature를 channel 별로 나누어 연산하는 것입니다. 최종적으로 group 별로 수행후 concat하여 다시 결합한 것이 출력이 됩니다. parameter 수를 g배로 줄일 수 있는데, 여기서 g는 group의 번호입니다. 아래의 그림은 group convolution의 예시입니다.</p>

<p><img src="../assets/img/WDSR/WDSR_02.png" alt="Alt text" /><br /></p>

<h3 id="-depthwise-separable-convolution-">[ Depthwise separable convolution ]</h3>

<p>Depthwise separable convolution는 각 channel 만의 spatial feature를 추출하는 것이 불가능하기 때문에 고안해낸 방법입니다. 속도 문제때문에 제안된 것은 아닙니다. 결과적으로 Depth-wise conv에서 하고자 하는 것은 각 channel 마다 spatial feature extraction하는 것이고 각 channel 마다 filter가 존재하고 때문에, input/output feature map size가 동일하다고 보시면 됩니다. 그리고, 위에서 설명한 group convolution에서 g를 channel 수 만큼 늘려준 것이라고 생각하시면 됩니다. 아래의 그림은 depthwise separable convolution의 예시입니다.</p>

<p><img src="../assets/img/WDSR/WDSR_03.png" alt="Alt text" /><br /></p>

<h3 id="-inverted-residuals-">[ Inverted residuals ]</h3>

:ET