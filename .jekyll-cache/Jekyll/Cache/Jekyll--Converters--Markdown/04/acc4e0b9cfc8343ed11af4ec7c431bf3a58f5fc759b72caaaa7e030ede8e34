I"$	<h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1803.02735 (CVPR 2018)</p>

<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>Feed-forward architecture는 저해상도와 고해상도 이미지의 상호의존 관계를 완전히 설명하지 못한다고 합니다. 
여기서 말하는 저해상도 이미지와 고해상도 이미지의 상호의존관계는 feed-forward 구조의 경우 어쨋든 input은 LR image 이며 모든 layer들이 LR image에서 extraction된 정보만 이용하기 때문에, LR image &lt;&gt; HR image 사이의 의존관계를 부정적으로 바라보고 있는 것 같습니다. 즉, 어느순간에는 LR image으 정보로만 HR image를 만들어 내는 것은 부적절 하다. 까지 바라볼 수 있었습니다.</p>

<blockquote> We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and downsampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually connected up- and down-sampling stages each of which represents different types of image degradation and highresolution components.
</blockquote>

<p>눈길을 끄는 단어는 *feedback mechanism, *projection error, *image degradation, *high-resolution componets. 입니다.</p>

<p>저자의 말에 따르면, 제안하고자 하는 네트워크에는 반복적인 Up/Down sampling layer를 포함하고, Up/Down sampling layer 들은 projection error를 계산하고 이를 공유하여 성능을 보이겠다. 라고 하는것으로 보입니다. 이를 (feedback mechanism) 이라고 하는 것 같습니다.</p>

<p>network에서 반복되는 output_Up(Upsample), output_Down(Downsample)이라면, output_UP를 HR중 하나로 보고, output_Down를 LR중 하나로 보고 있다고 생각이 됩니다.</p>

<p>핵심은, 이를 통해서 x8 (lager scale factor) 에서 성능을 보여줬다는 것입니다.</p>

<p>당시 Challenge (x8) track 에서 1위를 하였습니다.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>기본적으로 간단한 바</p>

:ET