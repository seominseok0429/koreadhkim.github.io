I"­(<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>https://arxiv.org/pdf/1903.09814 (CVPR 2019)</p>

<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>DBPNì—ì„œ ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì €ìëŠ” ê¸°ì¡´ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì€ HVS(human visual system)ì— ì¡´ì¬í•˜ëŠ” feedback mechanismì„ ì™„ì „íˆ í™œìš©í•˜ì§€ ëª»í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” low-level representationì„ high-level informationì„ ì´ìš©í•˜ì—¬ ê°œì„ í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë–„, feedback ë°©ì‹ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•˜ì—¬ hidden states (RNN)ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. feedback block ì€ feedback connectionì„ handeling í•  ìˆ˜ ìˆë„ë¡ designe ë˜ì—ˆê³ , ì´ëŠ” powerful high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ SRBFNì€ strong early reconstruction abilityë¥¼ ê°–ê³  ìˆìœ¼ë©°, step by step ìœ¼ë¡œ ìµœì¢… HR image ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” SR task ì—ì„œ curriculum learning ë°©ë²•ì„ í•™ìŠµì „ëµìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ì–´ë–»ê²Œ ì‚¬ìš©í•˜ì˜€ì§€ë„ êµ‰ì¥í•œ ê¶ê¸ˆì¦ì„ ìœ ë°œí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ì €ìëŠ” multiple type degradation taskì¸ SRì— ì í•©í•˜ë‹¤ê³  íŒë‹¨ í•˜ì˜€ìŠµë‹ˆë‹¤.
ë‹¤ìŒì˜ ê·¸ë¦¼ì€ RNN(Recurrent Neural Network)ì˜ hidden state ì…ë‹ˆë‹¤.<br />
<img src="/assets/img/SRFBN/SRFBN_02.png" alt="RNN hidden state" />
RNN ì˜ ê¸°ë³¸êµ¬ì¡°ì…ë‹ˆë‹¤. Green box = hidden state, Red box = input, Blue box = output ì…ë‹ˆë‹¤.
ì§ê´€ì ìœ¼ë¡œ ë³´ë©´, Green boxëŠ” inputê³¼ ë”ë¶ˆì–´ ì´ì „ term ì˜ Green boxë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ New Blue box(output)ì„ ìƒì„±í•œê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. CNNì—ì„œ ìƒê°í•´ë³´ë©´, layer ë¥¼ F(first) M(middel) F(final) ì´ë¼ê³  ìƒê°í•˜ë©´, Mt input = Ft + Mt-1 ì´ë¼ê³  ìƒê° í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>image super - resolution (SR) ì€ low-level computer vision task ì…ë‹ˆë‹¤. ëª¨ë“  ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ê²ƒ ì²˜ëŸ¼, SRì€ ì¼ë°˜ì ìœ¼ë¡œ 1 : N ì˜ ë‹µì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” Taskì…ë‹ˆë‹¤.(ill-posed problom), 1(LR) : N(HR) OR 1(HR) : N(LR), ë‘ ê²½ìš° ëª¨ë‘ í¬í•¨ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. 
ì´ì „ì— ì–´ë–¤ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì´ ìˆì—ˆëŠ”ì§€ ëŒ€ëµì ì¸ ë¶€ë¶„ì€ DBPN paper ì— ì˜ ì •ë¦¬ê°€ ë˜ì–´ìˆìŠµë‹ˆë‹¤.</p>

<p>ë‹¹ì—°íˆ network êµ¬ì¡°ê°€ ê¹Šì–´ì§ì— ë”°ë¼ parameterê°€ ì¦ê°€í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” overfitting ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.
parameterë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ recurrent structureë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. SR task ë„ ë§ˆì°¬ê°€ì§€ ì…ë‹ˆë‹¤. (eg.DRCN, DRRN) ì´ëŸ¬í•œ êµ¬ì¡°ì—ì„œ single-state Recurrent nuiral network(RNN)ìœ¼ë¡œ ì¶”ë¡ í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ deep learning based methodì™€ ìœ ì‚¬í•˜ê²Œ recurrent structure networkë„ feed-forwoar ë˜ë©´ì„œ ì •ë³´ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ feed-forward ë°©ì‹ì€ skip connectionì„ ì‚¬ìš©í•˜ë”ë¼ë„ previous layerì—ì„œ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” layer ì˜ useful informationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë„ ë‹¹ì—°í•œ ê²ƒì´ ì¼ë²ˆì ìœ¼ë¡œ F(First) M(middel) F(final) ì˜ layer ê°€ ìˆìœ¼ë©´ ì¼ë°˜ì ì¸ feed-forward êµ¬ì¡°ì—ì„œëŠ” F(first)ëŠ” inputì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  M(middel)ì€ Fout(first output)ì´ ì…ë ¥ì´ë¯€ë¡œ F(first)ëŠ” Mout(middle output)ì„ ì ‘ê·¼ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<p>ì €ìëŠ” congnition theory ì—ì„œ feedback connectionì´ ê³ ì°¨ì› ì˜ì—­ìœ¼ë¡œ ì €ì°¨ì›ì˜ì—­ìœ¼ë¡œ ì‘ë‹µ ì‹ í˜¸ë¥¼ ì „ì†¡í•œë‹¤.ë¼ê³  í•˜ëŠ”ë°, ì‰½ê²Œë§í•´ì„œ, high-level feature ë¥¼ low-level layerì— ì „ë‹¬ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤. DBPNì—ì„œëŠ” Up/Down sampling Unit ì„ í†µí•˜ì—¬ feedback mechanismì„ ì ìš©í•˜ì˜€ëŠ”ë°, ì´ëŠ” ê°„ë‹¨í•˜ê²Œ Upsampling featureë¥¼ ë‹¤ì‹œ Down sampling í•˜ì—¬ low-level ë¡œ projectioní•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” feed-back connectionì„ í†µí•˜ì—¬ high-level informationì„ low-level informationì„ refine í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ feed-back blockì„ ë³´ìœ í•˜ê³  ìˆëŠ” RNN êµ¬ì¡°ë¼ê³  í•©ë‹ˆë‹¤. í•´ë‹¹ network ëŠ” dense skip connectionì´ ìˆëŠ” ì—¬ëŸ¬ê°œì˜ Up/Down sampling ì„ í†µí•˜ì—¬ strong high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>[40]ì—ì„œ ì˜ê°ì„ ë°›ì•„ì„œ FBì˜ outputì— hidden state in an unfolded RNNì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤ (achieve the feedback manner)</p>

<p><img src="../assets/img/SRFBN/SRFBN_03.png" alt="Alt text" />
<img src="../assets/img/SRFBN/SRFBN_04.png" alt="Alt text" /></p>

<p>ìœ„ì˜ ê·¸ë¦¼ì„ ì„¸ë¡œë¡œ ì„¸ìš°ë©´ ìœ„ì—ì„œ ì–¸ê¸‰í•œ RNN êµ¬ì¡°ì™€ ë˜‘ê°™ë‹¤ ë¼ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì˜¤ë¥¸ìª½) ì´ë•Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Ft-1ì´ HR image informationì„ í¬í•¨í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ tain loss iteration ë§ˆë‹¤ loss connectí•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="../assets/img/SRFBN_01.png" alt="Alt text" /><br />
ì´ëŸ¬í•œ principle of feed-back scheme ëŠ” coarse SR image ê°€ better SR image ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡  LR image ë¥¼ refine í•´ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><strong><u>In summary, our main contributions are as follows:</u></strong><br /><br />
<strong><u>[1]</u></strong> : Feedback mechanism ì„ ì ìš©í•œ SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. networkëŠ” Feed-back connectionì„ í†µí•˜ì—¬ top-down feedback flowì—ì„œ high-level informationì„ ì œê³µí•©ë‹ˆë‹¤. parameterê°€ ê±°ì˜ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, strong early reconstruction abilityë¥¼ ì œê³µí•œë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p><strong><u>[2]</u></strong> : feed-back block ì„ ì œì•ˆí•©ë‹ˆë‹¤.(FB), í•´ë‹¹ blockì€ feedback information flowë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ 
handling í•  ìˆ˜ ìˆìœ¼ë©°, Up/Down sampling layer, dense skip connction ì„ í†µí•˜ì—¬, enriche high-level representationì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br /></p>

<p><strong><u>[3]</u></strong> : SR task ì—ì„œ curriculum training strategyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. SRì—ì„œ ì–´ë–»ê²Œ ë‚œì´ë„ë¥¼ ì¡°ì ˆí•˜ì˜€ëŠ”ì§€ê°€ ê¶ê¸ˆí•´ì§€ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.</p>

<h2 id="2-relatedwork">2. Relatedwork</h2>

<h3 id="-low-level-layer--high-level-layer-">[ low-level layer / high-level layer ]</h3>
<p><img src="../assets/img/SRFBN/SRFBN_05.png" alt="Alt text" />
ìœ„ì˜ VDSR êµ¬ì¡°ë¥¼ ë³´ë©´, ë‹¹ì—°íˆ low layerì˜ receptive field ëŠ” ì‘ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” SRFBNì˜ êµ¬ì¡°ëŠ” low-level layerê°€ high receptive fieldë¥¼ ë³´ëŠ” layer (high-level layer) ì˜ informationì— ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì´ low-level featureë¥¼ refine í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-feedback-mechanism-">[ Feedback mechanism ]</h3>
<p>Feed-back mechanismì€ previous layerì˜ ì •ë³´ë¥¼ refine í•˜ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” Feed-back mechanism ì„ ì—°êµ¬í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. [5,4,40,11,10,28] [11]ì€ ë³¸ ë¸”ë¡œê·¸ì— í¬ìŠ¤íŠ¸ ë˜ì–´ìˆëŠ” DBPN ì…ë‹ˆë‹¤.</p>

<p>ì €ìì˜ ë§ì— ë”°ë¥´ë©´,</p>
<blockquote> [11] designed up- and down-projection units to achieve iterative error feedback. Han et al. [10] applied a delayed feedback mechanism which transmits the information between two recurrent states in a dual-state RNN. However, the flow of information from the LR image to the final image is still feedforward in their network architectures unlike ours. </blockquote>
<p>DBPNì„ ë³´ë©´ Feedback mechanism ì„ ì‚¬ìš©í•œë‹¤ê³  í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ? Up/Down sampling ì„ í†µí•œ projectionìœ¼ë¡œ ë³´ê³  feed-foward ëœë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì ì—ì„œ LR to HR ì—ì„œ ì—¬ì „íˆ information flowì€ feed-fowardë˜ëŠ” ì ì„ ë‹¨ì ìœ¼ë¡œ ë³´ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p>ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ê³¼ ê´€ë ¨ì´ ê°€ì¥ ê¹Šì€ ì—°êµ¬ëŠ” [40]ì´ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ [40]ì—ì„œ ì„¤ëª…í•˜ëŠ” Feed-back based learning model ì…ë‹ˆë‹¤.
<img src="../assets/img/SRFBN/SRFBN_06.png" width="45%" height="45%" />
<img src="../assets/img/SRFBN/SRFBN_07.png" width="50%" height="50%" /><br /></p>

<p>ì €ìëŠ” ìƒë‹¨ìš°ì¸¡ ê·¸ë¦¼ì— í‘œì‹œëœ ConvLSTMì„ ì‚¬ìš©í•˜ì§€ë§Œ, SRFBNì˜ module(Feedback block(FB)) ë¡œì¨ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê² ë‹¤ê³  í•©ë‹ˆë‹¤. ë’¤ì—ì„œ ì‚´í´ë³´ê² ì§€ë§Œ, ConvLSTMë³´ë‹¤ SRì— ì í•©í•˜ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-curriculum-learning-">[ Curriculum learning ]</h3>
<p><img src="../assets/img/SRFBN/SRFBN_08.png" width="80%" height="80%" /><br />
ìœ„ì˜ ê·¸ë¦¼ìœ¼ë¡œ ì§ê´€ì ìœ¼ë¡œ ë³´ìë©´, í•™ìŠµëŒ€ìƒì˜ ë‚œì´ë„ë¥¼ ì ì  ë†’ì—¬ê°„ë‹¤ëŠ” ê²ƒì´ Curriculum learningì˜ ê¸°ì´ˆì…ë‹ˆë‹¤. ê·¸ë¦¼ì€ shapre recognitionì¸ë°, ì‰¬ìš´ sample ë¡œëŠ” ì •í™•í•œ ì›, ì •ì‚¬ê°í˜•ì„ ì‚¬ìš©í•˜ê³ , hard í•œ ìƒ˜í”Œë¡œëŠ” ì§ì‚¬ê°í˜•,íƒ€ì›ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” non-convexì—ì„œ ì§ˆì¢‹ì€ local-minimaë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì†Œê°œ ë˜ê³  ìˆê³ , ì´ëŠ” ë¹ ë¥¸ converge speedì™€ ì•ˆì •ì„±ì˜ ì´ì ì„ ê°€ì ¸ì˜¨ë‹¤ê³  ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ, SRì€ í•™ìŠµì´ ë¶ˆì•ˆì •í•œ Task ì¤‘ì— í•˜ë‚˜ì´ë©°, Adam optimizerë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤. ì´ëŸ°ì ì—ì„œ curriculum learningì´ SR domainì— ì í•©í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°ë©ë‹ˆë‹¤.</p>

<p>ì´ì™€ ê°™ì€ í•™ìŠµì „ëµì„ SR Task ì— ì ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. Curriculum learning ì—ì„œëŠ” ì‰¬ìš´ sample ì„ ì •ì˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ 2ê°€ì§€ë¥¼ ì œì‹œí•œ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. (1) ë…¸ì´ì¦ˆì˜ ê°œìˆ˜ë¡œ íŒë‹¨. (2) ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ë°”ìš´ë”ë¦¬ì—ì„œ margin ê±°ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. margin ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ ìˆ˜ë¡ ì‰½ê³ , margin ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ì–´ë µë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>SR task ì—ì„œëŠ” LR -&gt; HR ë¡œ non-linear mappingí•˜ëŠ” ê²ƒì¸ë°, HRì— noiseê°€ ìˆë‹¤ë˜ì§€ ë­”ê°€ LRí•˜ê³  ì¡°ê¸ˆì´ë¼ë„ ë¹„ìŠ·í•´ì§€ë©´ ë‹¹ì—°íˆ easy sample ì´ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œë„ easy sample ë¡œ HR(noise)ì„ ì‚¬ìš©í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h2 id="3-feedback-network-for-image-sr">3. FeedBack Network for Image SR</h2>
<p>Two requirements are contained in a feedback system : <br />
<strong><u>(1) iterativeness</u></strong> <br />
<strong><u>(2) rerouting the output of the system to correct the input in each loop</u></strong></p>

<h3 id="-up-projection-uint-">[ Up-Projection Uint ]</h3>

<p>[DBPN] : https://github.com/thstkdgus35/EDSR-PyTorch â€œIncludes implementation of DBPNâ€</p>

:ET