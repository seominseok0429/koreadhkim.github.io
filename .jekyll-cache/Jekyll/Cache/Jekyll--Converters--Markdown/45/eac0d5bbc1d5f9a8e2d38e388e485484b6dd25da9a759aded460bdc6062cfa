I"ö9<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>https://arxiv.org/pdf/1808.08718.pdf (CVPR 2018, NTIRE18 Champion)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>

<h3 id="1-ë…¼ë¬¸ì—ì„œ-same-parameter--computational-budgetìœ¼ë¡œ-ë”-ë‚˜ì€-ì„±ëŠ¥ì„-ê°€ì§€ëŠ”-model-ì„-ì œì•ˆí•˜ê² ë‹¤">[1] ë…¼ë¬¸ì—ì„œ same parameter / computational budgetìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” Model ì„ ì œì•ˆí•˜ê² ë‹¤.</h3>
<h3 id="2-1ì„-ìœ„í•œ-ë°©ë²•ìœ¼ë¡œ-residual-network-ì—ì„œ-activation-function-ì „ì—-activation-mapì„-ëŠ˜ë¦°ë‹¤-x2-x4-ì´ëŠ”-slim-identity-pathwayë¥¼-ê°–ëŠ”ë‹¤">[2] [1]ì„ ìœ„í•œ ë°©ë²•ìœ¼ë¡œ residual network ì—ì„œ activation function ì „ì— activation mapì„ ëŠ˜ë¦°ë‹¤. (x2, x4) ì´ëŠ” slim identity pathwayë¥¼ ê°–ëŠ”ë‹¤.</h3>
<h3 id="3-2ì—ì„œ-ë”-ë‚˜ì•„ê°€-x6-x9ë¡œ-í™•ëŒ€í•˜ê¸°-ìœ„í•´ì„œ-linear-low-rank-convolutionì„-ì‚½ì…í•˜ì—¬-accuracy-efficiency-tradeoffë¥¼-í•´ê²°í•œë‹¤">[3] [2]ì—ì„œ ë” ë‚˜ì•„ê°€ (x6, x9)ë¡œ í™•ëŒ€í•˜ê¸° ìœ„í•´ì„œ linear low-rank convolutionì„ ì‚½ì…í•˜ì—¬ accuracy-efficiency tradeoffë¥¼ í•´ê²°í•œë‹¤.</h3>
<h3 id="4-bn-or-no-bnì„-ë¹„êµí•œë‹¤-weight-normalizationì´-ë”-ì¢‹ì€-ì„±ëŠ¥ì„-ë³´ì´ëŠ”-ê²ƒì„-í™•ì¸í•œë‹¤">[4] BN or no BNì„ ë¹„êµí•œë‹¤. weight normalizationì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•œë‹¤.</h3>

<h2 id="1-introduction">1. Introduction</h2>
<p>ë‹¤ìˆ˜ì˜ SRë…¼ë¬¸ì—ì„œ (eg.SISR) Introductionì—ì„œ ë§í•˜ëŠ” ë‚´ìš©ì€ ë¹„ìŠ·í•©ë‹ˆë‹¤.(ì œê°€ ì½ì€ SISRì— í•œí•´ì„œ)<br />
(1) LR â€“&gt; HR Taskì´ë‹¤.<br />
(2) ill-posed problemì´ë‹¤.<br />
(3) ì´ëŸ¬í•œ issue ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§ì€ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆë‹¤.<br />
(4) (3)ì—ì„œ ë§í•œ ë§ì€ ë°©ë²•ë“¤ì— ëŒ€í•œ ë§›ë³´ê¸°.<br />
(5) (4)ì—ì„œ ë§í•œ ë§›ë³´ê¸°ì—ì„œ issueë“¤ì„ ë‚˜ì—´<br /></p>

<hr />

<p>**[1] : **</p>

<h2 id="2-related-work">2. Related Work</h2>
<p><strong>channel attention(CA) :</strong> CAëŠ” high-level vision taskì—ì„œëŠ” ìì£¼ ì‚¬ìš©ë˜ì§€ë§Œ, low-level vision taskì—ì„œëŠ” ê±°ì˜ ì—°êµ¬ë˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ë•Œë¬¸ì—, ì €ìëŠ” SR Taskì˜ space limitationì— ë”°ë¼ì„œ, CNN-based methods / attention mechanismì— focus ë¥¼ ë§ì¶¥ë‹ˆë‹¤.<br /></p>

<p><strong>Predefined upsampling :</strong> í•´ë‹¹ ë°©ë²•ì€ LR â€“&gt; HR space ë¡œ mappingí•œ ë’¤ì—, í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ê³„ì‚°ëŸ‰ì— ëŒ€í•œ issueë„ ìˆì„ ë¿ë”ëŸ¬, interpolationìœ¼ë¡œ ì¸í•´ ì–»ëŠ” íš¨ê³¼ëŠ” resolutionì´ ì¦ê°€í•˜ëŠ”ê²ƒ ë°–ì— ì—†ìœ¼ë¯€ë¡œ ìµœê·¼ì—ëŠ” í•´ë‹¹ ë°©ë²•ì€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì €ìë„ ê°™ì€ ì´ì•¼ê¸°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p><strong>Thinking about Deep Network :</strong> ì €ìëŠ” EDSR/MDSRì„ ì˜ˆì‹œë¡œ ì„¤ëª…í•˜ê³  ìˆê³ , ë¬¼ë¡  ê¸°ì¡´ì˜ SRResNetêµ¬ì¡°ì—ì„œ ë¶ˆí•„ìš”í•œ ëª¨ë“ˆ(BN/ReLU)ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥í–¥ìƒì„ ì´ë£¨ì—ˆì§€ë§Œ, EDSR/MDSRì€ ë‹¤ë¥¸ networkì— ë¹„í•´ì„œ parameterìˆ˜ê°€ êµ‰ì¥íˆ ë§ì€ í¸ì´ë©°, residual blockì„ ê¹Šê²Œë§Œ ìŒ“ëŠ” ê²ƒì€ ìœ ì˜ë¯¸í•œ ë°©ë²•ì´ ì•„ë‹ˆë‹¤.ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ, Introducionì—ì„œ ë§í•œ, Issue 02ì— ëŒ€í•´ì„œ í•œë²ˆë” ë¶€ì •ì ì¸ ê²¬í•´ë¥¼ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h3 id="-attention-mechansim-">[ Attention mechansim ]</h3>
<p>ì¼ë°˜ì ìœ¼ë¡œ, Attention mechansimì€ ì„±ëŠ¥ì´ í–¥ìƒí•˜ëŠ” ìª½ìœ¼ë¡œ, biasë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
ìœ„ì—ë„ ë§í–ˆë“¯ì´ ì €ìëŠ” Attention mechansimì´ SR Taskì— ì„ í–‰ì—°êµ¬ë˜ì§€ ì•Šì•˜ê³  ì´ë¥¼ ì ìš©í•´ì„œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥í–¥ìƒì„ ì´ëŒì–´ë‚¸ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ, high-frequency featureê°€ HR reconstructionì— ìœ ìš©í•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¹ì—°í•œ ê²ƒì…ë‹ˆë‹¤. ì˜ë¬¸ì„ ê°€ì§ˆ ì—¬ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ ì €ìëŠ” networkê°€ ì´ëŸ¬í•œ(useful feature)ì— ë” attentioní•œë‹¤ë©´ ì¢‹ì€ ê°œì„ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="3-residual-channel-attention-network-rcan">3. Residual Channel Attention Network (RCAN)</h2>

<h3 id="-network-architecture-">[ Network Architecture ]</h3>
<p><img src="../assets/img/RCAN.png" alt="Alt text" /><br /></p>
<h4 id="ilr--input">I<sub>LR</sub> = input</h4>
<h4 id="isr--output">I<sub>SR</sub> = output</h4>
<p><img src="../assets/img/RCAN/RCAN_02.png" alt="Alt text" /><br /></p>
<h4 id="hsf--feature-extraction">H<sub>SF</sub> = feature extraction</h4>
<p><img src="../assets/img/RCAN/RCAN_03.png" alt="Alt text" /><br /></p>
<h4 id="f0--deep-feature-extrationì—-ì‚¬ìš©ë©ë‹ˆë‹¤-rir-modeul-ì˜-inputì´-ë©ë‹ˆë‹¤">F<sub>0</sub> = deep feature extrationì— ì‚¬ìš©ë©ë‹ˆë‹¤. RIR modeul ì˜ inputì´ ë©ë‹ˆë‹¤.</h4>
<p><img src="../assets/img/RCAN/RCAN_04.png" alt="Alt text" /><br /></p>
<h4 id="fdf--deep-featureë¡œ-ê°„ì£¼í•˜ê³ -ì´ëŠ”-upscale-module-ì—-input-ì…ë‹ˆë‹¤">F<sub>DF</sub> = deep featureë¡œ ê°„ì£¼í•˜ê³ , ì´ëŠ” upscale module ì— input ì…ë‹ˆë‹¤.</h4>
<p><img src="../assets/img/RCAN/RCAN_05.png" alt="Alt text" /><br /></p>
<h4 id="hrec--upscaling--convupscaling--network-architectureì™€-ê°™ì´-ë³´ì‹œë©´-ì‰½ê²Œ-ë”°ë¼ê°ˆ-ìˆ˜-ìˆìŠµë‹ˆë‹¤">H<sub>REC</sub> = Upscaling â€“&gt; Conv(Upscaling),  Network architectureì™€ ê°™ì´ ë³´ì‹œë©´ ì‰½ê²Œ ë”°ë¼ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</h4>

<h3 id="-loss-function-">[ Loss-function ]</h3>
<p><img src="../assets/img/RCAN/RCAN_07.png" alt="Alt text" /><br /></p>

<p>ì €ìëŠ”, ì´ì „ì˜ ë…¼ë¬¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ RCANì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•˜ì—¬ ì´ì „ì˜ ì—°êµ¬ë“¤ì—ì„œ ì‚¬ìš©í•œ ë°©ì‹ì¸ L1-lossë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-residual-in-residual-rir-">[ Residual in Residual (RIR) ]</h3>
<p><img src="../assets/img/RCAN/RCAN_12.png" alt="Alt text" /><br /></p>

<p>EDSRì—ì„œ residual block, skip connectionì„ ì´ìš©í•˜ì—¬ Deep CNNì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì…ì¦ë¬ë‹¤ê³  í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, residual êµ¬ì¡°ë¡œ ì¸í•˜ì—¬ 400ê°œ ì´ìƒì˜ ë§ì€ layerë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.<br />
g-th group ìœ¼ë¡œ ì´ë£¨ì–´ì§„, RGëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™” ëœë‹¤ê³  í•©ë‹ˆë‹¤.<br />
<img src="../assets/img/RCAN/RCAN_09.png" alt="Alt text" /><br /></p>
<h4 id="hg--g-th-rg">H<sub>g</sub> = g-th RG</h4>
<h4 id="fg-1--input-for-g-th-rg">F<sub>g-1</sub> = input for g-th RG</h4>
<h4 id="fg--output-for-g-th-rg">F<sub>g</sub> = output for g-th RG</h4>
<p>ì €ìëŠ” simple í•˜ê²Œ RG ë§Œ ë§ì´ ìŒ“ëŠ”ë‹¤ê³ í•´ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒì€ í™•ì¸í•˜ì§€ ëª»í•˜ì˜€ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ LSCë¥¼ RIRì— ì¶”ê°€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, trainingì„ ì•ˆì •í™”í•˜ë©´ì„œ ì„±ëŠ¥í–¥ìƒì„ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="../assets/img/RCAN/RCAN_10.png" alt="Alt text" /><br /></p>
<h4 id="wlsc--rir-conv-layer-weight-tail--biasx">W<sub>LSC</sub> = RIR conv layer weight (tail) , bias(x)</h4>

<p>ê²°ê³¼ì ìœ¼ë¡œ, RCAB(b-th residual, g-th RG)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™” ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<img src="../assets/img/RCAN/RCAN_11.png" alt="Alt text" /><br /></p>
<h4 id="fgb-1--g-th-rg-ì—ì„œ-b-th-rcab-input">F<sub>g,b-1</sub> = g-th RG ì—ì„œ b-th RCAB input</h4>
<h4 id="fgb--g-th-rg-ì—ì„œ-b-th-rcab-output">F<sub>g,b</sub> = g-th RG ì—ì„œ b-th RCAB output</h4>
<p>ì €ìëŠ” LR input / fetures ì—ëŠ” abundant informationì´ ë§ê³ , SR network ì˜ ëª©í‘œëŠ” useful informationì„ recover í•˜ëŠ” ê²ƒì´ë‹¤. ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. abundant information(low-frequency)ëŠ” identity-based skip connectionìœ¼ë¡œ ì „ë‹¬ ëœë‹¤ê³ í•©ë‹ˆë‹¤. residual lerningì„ ìœ„í•´ì„œ B residual channel attention blockì„ ìŒ“ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ë•Œ, B residual channel attention blockì€ ê°ê°ì˜ RGì— í¬í•¨ëœë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p><img src="../assets/img/RCAN/RCAN_13.png" alt="Alt text" /><br /></p>

<h3 id="-channel-attentionca-">[ Channel Attention(CA) ]</h3>

<p><img src="../assets/img/RCAN/RCAN_14.png" alt="Alt text" /><br /></p>

<p>ì €ìëŠ” feature map ì—ì„œ channel ê°„ì— ìƒí˜¸ì˜ì¡´ì„±ì„ ì´ìš©í•˜ì—¬ CAë¥¼ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.<br />
ê° channel ë³„ë¡œ different attentioní•˜ëŠ” ê²ƒì´ key step ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë•Œ, ë‘ê°€ì§€ concernsê°€ ìˆìŠµë‹ˆë‹¤.<br /></p>

<p><strong>First :</strong> LR space ëŠ” ë‹¹ì—°íˆ abundant low-frequency information, valuable high-frequency componetsë¥¼ ê°–ìŠµë‹ˆë‹¤. low-frequency partëŠ” more complanteí•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  high-frequency componetsëŠ”  ì¼ë°˜ì ìœ¼ë¡œ, edge, texture, detail informationì„ ê°–ìŠµë‹ˆë‹¤.<br />
<strong>Second :</strong>ê°ê°ì˜ Conv layer ëŠ” local receptive field ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, Conv ëª…ë ¹ì„ ìˆ˜í–‰í•œ ë’¤, outputì€ unable to exploit contextual inforamtion outside of the local region í•œ ê²ƒì€ ë‹¹ì—°í•œ ê²ƒ ì…ë‹ˆë‹¤.<br /></p>

<p>ì €ìëŠ” First, Second ì— ë¶„ì„ì— ê¸°ì´ˆí•˜ì—¬, global average pooling ì„ ì‚¬ìš©í•˜ì—¬ channel-wise global spatial informationì„ ê°€ì ¸ì˜¨ë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p>GAP (global average pooling)ì€ HxW pooling ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¦‰, í•œ feature map (HxWxC)ë¥¼ ì „ë¶€ pooling í•˜ì—¬ í•˜ë‚˜ì˜ neuron(1x1xC)ë¡œ mapping í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.
ì—¬ê¸°ì„œ GAP ì´í›„ì— softmaxë¥¼ ì·¨í•˜ë©´, ê° feature map ì˜ ì¤‘ìš”ë„(?)ê°€ ê³„ì‚°ë  ìˆ˜ ìˆì„ê²ƒì´ë¼ê³  ì˜ˆìƒì´ ë©ë‹ˆë‹¤.<br /></p>

<p><img src="../assets/img/RCAN/RCAN_15.png" alt="Alt text" /><br /></p>

<h4 id="x--x1-xc-xc-c--feature-map">X = [x1, â€¦,x<sub>c</sub>,â€¦ x<sub>C</sub>], C = feature map</h4>
<h4 id="hgp-gp--global-pooling">H<sub>GP</sub>, GP = Global pooling</h4>
<h4 id="zc-channel-statistic-local-descriptors">Z<sub>c</sub>, Channel statistic (local descriptors)</h4>
<p>ë” ì •êµí•œ ë°©ë²•ì´ ë„ì…ë  ìˆ˜ ìˆë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. <br /></p>

<p>ì´ì œ, GAPë¥¼ í†µí•´ì„œ aggregate ëœ ì •ë³´ë¡œ ë¶€í„° channel-sise dependencyë¥¼ ì•Œê¸° ìœ„í•´ì„œ Gating mechanismì„ ì†Œê°œí•œë‹¤ê³  í•©ë‹ˆë‹¤.<br />
Gating mechanismì„ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ì˜ ê¸°ì¤€ì´ ë§Œì¡±ë˜ì–´ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. <br />
<strong>First :</strong> channel ì‚¬ì´ì˜ nonlinear interactionì„ learningí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.<br />
<strong>Second :</strong> Ont-hot activation ê°™ì´ í•˜ë‚˜ë§Œ ê°•ì¡°í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ê°ê°ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.<br /></p>

<p>ì €ìëŠ”, First, Secondë¥¼ ë”°ë¼ì„œ, sigmoid functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
<img src="../assets/img/RCAN/RCAN_16.png" alt="Alt text" /><br /></p>

<h4 id="where-f--and-Î´--denote-the-sigmoid-gating-and-relu-34-function-respectively-wd-is-the-weight-set-of-a-conv-layer-which-acts-as-channel-downscaling-with-reduction-ratio-r-after-being-activated-by-relu-the-low-dimension-signal-is-then-increased-with-ratio-r-by-a-channel-upscaling-layer-whose-weight-set-is-wu--then-we-obtain-the-final-channel-statistics-s-which-is-used-to-rescale-the-input-xc-ì…ë‹ˆë‹¤ì›ë¬¸ë‚´ìš©-">where f (Â·) and Î´ (Â·) denote the sigmoid gating and ReLU [34] function, respectively. WD is the weight set of a Conv layer, which acts as channel-downscaling with reduction ratio r. After being activated by ReLU, the low-dimension signal is then increased with ratio r by a channel-upscaling layer, whose weight set is WU . Then we obtain the final channel statistics s, which is used to rescale the input xc ì…ë‹ˆë‹¤.(ì›ë¬¸ë‚´ìš©) <br /></h4>
<p>ê°„ë‹¨í•˜ê²Œ, input x<sub>c</sub>ì„ ë°›ì•„ì„œ, ReLUë¥¼ ì·¨í•œ ë’¤, W<sub>U</sub>ë§Œí¼, upscaling ì„ í•©ë‹ˆë‹¤. (weight scaling) ê·¸ ë’¤, Sigmoid functionì„ ì‚¬ìš©í•˜ì—¬, rescaling ëœ input x<sub>c</sub>ë¥¼ ì–»ì–´ëƒ…ë‹ˆë‹¤. rescaling ëœ  x<sub>c</sub> = s ì…ë‹ˆë‹¤.<br />
<img src="../assets/img/RCAN/RCAN_17.png" alt="Alt text" /><br />
ìœ„ì˜ ì‹ì„ ë³´ì‹œë©´ ê²°ê³¼ì ìœ¼ë¡œ, ì¢Œí•­ì— ìˆëŠ”ê²ƒì€, feature map ì—ì„œ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬ rescaling í•´ì¤€ ê²ƒì´ ë  ê²ƒê°™ìŠµë‹ˆë‹¤. channel attentionì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, RCABì—ì„œ residual ë¶€ë¶„ì´ adaptively í•˜ê²Œ rescaling ëœë‹¤ê³  í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ìƒí™©ì— ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<p><img src="../assets/img/RCAN/RCAN_19.png" alt="Alt text" /><br />
<img src="../assets/img/RCAN/RCAN_20.png" alt="Alt text" /><br /></p>

<h3 id="-residual-channel-attention-block-rcab-">[ Residual Channel Attention Block (RCAB) ]</h3>
<p><img src="../assets/img/RCAN/RCAN_18.png" alt="Alt text" /><br />
CAë¥¼ RCABì— ì‚½ì…í–ˆë‹¤. ë¼ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì´ ì •í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="4-experiments">4. Experiments</h2>

<h3 id="-experiment-preveiw-">[ Experiment preveiw ]</h3>

<p>training dataset = DIV2K<br />
benchmark = Set5, Set14, B100, Urban100, Manga109<br />
ì´ì „ networkì™€ ë™ë“±ë¹„êµ í•˜ê¸° ìœ„í•˜ì—¬ Y(luminance) channel ì—ì„œ ë§Œ evaluation í•©ë‹ˆë‹¤. <br /></p>

<p>BD : Gaussian blur â€“&gt; Down scaling</p>

<p>Training settings : 90,180,270 rotation, horizontal flip<br />
patch size : 48x48<br />
batch size : 16<br /></p>

<p>optimizer : Adam<br />
hardware : NVIDIA Titan Xp<br /></p>

<h3 id="-experiment-effects-of-rir-and-ca-">[ Experiment Effects of RIR and CA ]</h3>

<p><img src="../assets/img/RCAN/RCAN_21.png" alt="Alt text" /><br /></p>

<p>ìœ„ì˜ Table ì´ ì‹¤í—˜ ê²°ê³¼ì…ë‹ˆë‹¤. í â€¦ ì‚¬ì‹¤ ì¡°ê¸ˆ ì•„ì‰½ìŠµë‹ˆë‹¤. CAë¥¼ ì •êµí•˜ê²Œ í•˜ì§€ ì•Šì•„ì„œ ì¸ì§€, ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì´ìœ ì¸ì§€.. ì €ìê°€ ë§í•œê²ƒ ë§Œí¼ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ LSC,SSCê°€ ì„±ëŠ¥ì— ë§ì€ ë³€í™”ë¥¼ ì£¼ì§€ëŠ” ëª»í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì² ì €íˆ ê°œì¸ì ì¸ ìƒê°ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, training ë¶ˆì•ˆì •ì„±ì„ í•´ê²°í•˜ê³ , converge speedê°€ ë¹ ë¥¸ ê²ƒì€ ë‹¹ì—°í•œ ì‚¬ì‹¤ì¼ê²ƒì…ë‹ˆë‹¤. 0.45 dbì˜ ì„±ëŠ¥ë³€í™” í­ì´ ë‚®ë‹¤. ë¼ëŠ” ë§ì€ ê²°ì½” ì•„ë‹™ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, í•´ë‹¹ ë…¼ë¬¸ì—ì„œ parameter ìˆ˜ê°€ ê·¹ì•…ì…ë‹ˆë‹¤. SRì—ì„œ ë§ë‹¤ê³  ë³´ëŠ” EDSRë³´ë‹¤ 3ë°°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ëŠ” multi scaleì´ ê°€ëŠ¥í•œ MDSR ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤â€¦<br /> (ë‹¤ì‹œí•œë²ˆ ë§ì”€ë“œë¦¬ì§€ë§Œ, ì² ì €íˆ ê°œì¸ì ì¸ ì˜ê²¬ì…ë‹ˆë‹¤.)</p>

<h3 id="-experiment-results-with-bicubic-bi-degradation-model-">[ Experiment Results with Bicubic (BI) Degradation Model ]</h3>
<p><img src="../assets/img/RCAN/RCAN_24.png" alt="Alt text" /><br /></p>

<h3 id="-experiment-object-recognition-performance-">[ Experiment Object Recognition Performance ]</h3>
<p><img src="../assets/img/RCAN/RCAN_25.png" alt="Alt text" /><br />
Object Recognition ì—ì„œ preprocessing ì—ì„œë„ ê¸°ì¡´ì˜ ë°©ë²•ë“¤ ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.<br /></p>

<h3 id="-experiment-benchmark-">[ Experiment benchmark ]</h3>

<p><img src="../assets/img/RCAN/RCAN_22.png" alt="Alt text" /><br /></p>

<h3 id="-experiment-visualize-">[ Experiment visualize ]</h3>

<p><img src="../assets/img/RCAN/RCAN_23.png" alt="Alt text" /><br /></p>
:ET