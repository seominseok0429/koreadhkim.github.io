I"ü=<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>https://arxiv.org/pdf/1807.02758.pdf (ECCV 2018)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>SRì—ì„œ network ì˜ deepí• ìˆ˜ë¡ ì¢‹ë‹¤.! ë¼ëŠ” ê²ƒì€ VDSRì„ í†µí•´ì„œ ë§ì´ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ê¸°ë³¸ì ìœ¼ë¡œ networkê°€ deepí• ìˆ˜ë¡ í•™ìŠµì´ ì–´ë ¤ìš´ê²ƒì€ ë³´í¸ì ì¸ ì‚¬ì‹¤ì…ë‹ˆë‹¤. ë•Œë¬¸ì—, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë§ì€ í•™ìŠµê¸°ë²•ë“¤ì´ ì œì•ˆë˜ì–´ ì™”ìŠµë‹ˆë‹¤. classificationê³¼ ê°™ì€ taskëŠ” í•™ìŠµì´ ë¹„êµì  ì‰½ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, SR taskëŠ” í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë‹¤.(ê¹Œë‹¤ë¡­ë‹¤.)ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì €ìëŠ” SR taskì—ì„œ networkê°€ deep í• ìˆ˜ë¡ trainì— ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ì œì•ˆëœ ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ë…¼ë¬¸ë“¤ì€ ëŒ€ë¶€ë¶„ ì œì•ˆí•œ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ê¸°ë²•ì„ ê°™ì´ ì œì•ˆí•˜ëŠ”ì‹ìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤.) ë”°ë¼ì„œ, ë…¼ë¬¸ì€ ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ RCANì„ ì œì•ˆí•œë‹¤ê³  í•©ë‹ˆë‹¤. very deep networkë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„œ Residual in Residual (RIR)ì„ ì œì•ˆ í•œë‹¤ê³  í•©ë‹ˆë‹¤. RIRì€ ëª‡ê°œì˜ residual groupsìœ¼ë¡œ êµ¬ì„±ë˜ë©° long skip connectionì„ ê°–ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.
ì €ìëŠ” ë˜í•œ, low-resolution image input ê³¼ feature ì—ëŠ” channelë³„ë¡œ ë™ë“±í•˜ê²Œ ì—¬ê²¨ì§€ëŠ” abundant low-frequency informationì„ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì— CNN êµ¬ì¡°ì˜ ì¥ì ì¸ representational ability ë¥¼ ì œí•œí•œë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ë…¼ë¬¸ì—ì„œëŠ” channel ì‚¬ì´ ìƒí˜¸ì˜ì¡´ì„±ì„ ê³ ë ¤í•˜ì—¬ channel attention mechanismì„ ì œì•ˆí•©ë‹ˆë‹¤.<br /></p>

<p>CNNì—ì„œ ë§í•˜ëŠ” channel attention mechanismì„ ì‰½ê²Œ ì´í•´í•˜ê³  ê°€ë ¤ë©´ ë‹¤ìŒê·¸ë¦¼ì„ ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>ë‹¤ìˆ˜ì˜ SRë…¼ë¬¸ì—ì„œ (eg.SISR) Introductionì—ì„œ ë§í•˜ëŠ” ë‚´ìš©ì€ ë¹„ìŠ·í•©ë‹ˆë‹¤.(ì œê°€ ì½ì€ SISRì— í•œí•´ì„œ)<br />
(1) LR â€“&gt; HR Taskì´ë‹¤.<br />
(2) ill-posed problemì´ë‹¤.<br />
(3) ì´ëŸ¬í•œ issue ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§ì€ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆë‹¤.<br />
(4) (3)ì—ì„œ ë§í•œ ë§ì€ ë°©ë²•ë“¤ì— ëŒ€í•œ ë§›ë³´ê¸°.<br />
(5) (4)ì—ì„œ ë§í•œ ë§›ë³´ê¸°ì—ì„œ issueë“¤ì„ ë‚˜ì—´<br /></p>

<hr />

<p>ë§ì€ ë¶„ë“¤ì´ ì•„ì‹œë‹¤ì‹œí”¼, residualì„ ì˜ë¯¸ì—†ì´ ë§ì´ ì‚½ì…í•˜ê³ , deep networkë¥¼ êµ¬ì¶•í•œë‹¤ê³  í•´ì„œ ì„±ëŠ¥ì´ ìœ ì˜ë¯¸í•˜ê²Œ ì¢‹ì•„ì§€ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì •êµí•œ residualì´ ì„±ëŠ¥ì„ ìœ ì˜ë¯¸í•˜ê²Œ í–¥ìƒì‹œí‚¬ê²ƒì…ë‹ˆë‹¤. ì €ìë„ ìœ ì‚¬í•œ ì´ì•¼ê¸°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p><strong>issue 01 :</strong> residual ì„ ì˜ë¯¸ì—†ì´ ë§ì´ ì‚½ì…í•˜ê³ , Deep networkë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ì‹<br />
<strong>issue 01-related problem:</strong><br />
(1) parameter increment / a fall in computational efficiency.
(2) (1)ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì„±ëŠ¥í–¥ìƒí­ì€ ë‚®ìŒ.</p>

<p><strong>issue 02 :</strong> [1~11] ì—ì„œ ì œì•ˆëœ ë…¼ë¬¸ë“¤ì€ channelwise features ë¥¼ ë™ë“±í•˜ê²Œ ë‹¤ë£¨ê³  ìˆë‹¤.<br />
<strong>issue 02-related problem:</strong><br />
(1) low/high frequency informationì´ ì„œë¡œ dealing í•˜ëŠ”ë° ìˆì–´ì„œ flexibilityê°€ ê²°ì—¬ëœë‹¤.<br />
(2) wastes unnecessary computations(low-frequency ê´€ì )<br />
(3) discriminative ability leanring X (channel ê´€ì ) â€“&gt; deep networkì˜ representational power ë¥¼ ë‚®ì¶¤.</p>

<p><strong>Therefore, proposed RCAN :</strong> residualì„ ì˜ë¯¸ìˆê²Œ ê°€ì ¸ê°€ë©´ì„œ, channelwise featuresë¥¼ ì°¨ë³„ì„±ì„ ë‘ê³  ë‹¤ë£¨ê² ë‹¤. í’€ì–´ì“°ë©´, ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.<br />
LR â€“&gt; Network â€“&gt; HR ì—ì„œ, LRì€ High-frequency information ë³´ë‹¤ low-frequnecy informationì´ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´, NetworkëŠ” ê²°êµ­ LRì—ì„œ ë¶€ì¡±í•œ ì •ë³´ì¸ high-frequency inforamtionì„ restoreí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë  ê²ƒ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ì´ë•Œ, LRì— í¬í•¨ëœ Low-frequency information ë„ ìƒì–´ë²„ë¦¬ì§€ ì•Šê¸° ìœ„í•´ residualì„ ì–´ë–»ê²Œ ì“°ëŠëƒì— ëŒ€í•œ ì—°êµ¬ë„ ë§ì´ ìˆìŠµë‹ˆë‹¤. ì €ìëŠ” ê¹Šì€ networkì—ì„œ low-frequency inforamtionì„ computeí•˜ëŠ”ë° ë§ì€ ë‚­ë¹„ë¥¼ í•œë‹¤ê³  ë§í•˜ê³  ìˆìœ¼ë©°, feature channelsì„ discriminative í•˜ê²Œ learning í•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤ê³  ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ë¬¸ì œì ë“¤ì€ HR(output)ì´ representationí•˜ëŠ”ë° ì•…ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤. ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.<br /></p>

<p><strong>Approximate RCAN information :</strong> RIR(RG(SSC),LSC)<br />
í’€ì–´ì“°ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. RIR(residual in residual)êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ë©°, RIRì€ residual group(RG)ì™€ long-skip-connection(LSC)ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ë•Œ, RGëŠ” short-skip-connection(SSC)ë¥¼ ì‚¬ìš©í•˜ì—¬, EDSRì˜ residual blockì„ ìŒ“ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤. LSC, SSCê°€ í¬í•¨ë¨ìœ¼ë¡œì¸í•´ low-frequency informationì„ ì˜ bypassí•  ìˆ˜ìˆìœ¼ë©°, information flowë¥¼ ì›í™œí•˜ê²Œ ê°€ì ¸ê°„ë‹¤ê³  í•˜ê³ ìˆìŠµë‹ˆë‹¤.<br /></p>

<p>ë˜í•œ, Issue 02 ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ ì €ìëŠ” channel attention(CA) mechanismì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì§ê´€ì ìœ¼ë¡œ ì •ë‹µì— ê·¼ì‚¬í•˜ê¸°ìœ„í•´, Lossë¥¼ ì¤„ì´ê¸° ìœ„í•´ useful featureë¥¼ ê³ ë¥´ê² ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ë³´ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br /></p>

<p><strong>Overall, our contributions are three-fold:</strong><br />
<strong><u>[1]</u></strong> : ì„±ëŠ¥ ë†’ì€ SRì„ ìœ„í•˜ì—¬ RCANì„ ì œì•ˆí•˜ë©° ì´ëŠ” ì´ì „ì˜ ì œì•ˆëœ ë°©ë²•ë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.<br />
<strong><u>[2]</u></strong> : Deep-Network trainingì´ ê°€ëŠ¥í•œ RIRêµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. RIRë‚´ë¶€ì— ì¡´ì¬í•˜ëŠ” SSC/LSCëŠ” networkê°€ low-frequency informationì„ ì˜ ì „ë‹¬í•˜ë©°, networkê°€ ë” íš¨ê³¼ì ì¸ ì •ë³´ë¥¼ trainingí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.<br />
<strong><u>[3]</u></strong> : CA(channel Atteation) mechanismì„ ì œì•ˆí•˜ê³  CAë¡œ ì¸í•˜ì—¬ networkì˜ representational ability ë¥¼ í–¥ìƒ ì‹œí‚µë‹ˆë‹¤. <br /></p>

<p>ì •ì„±ì ì¸ ì„±ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.<br />
<img src="../assets/img/RCAN/RCAN_01.png" alt="Alt text" /><br /></p>

<h2 id="2-related-work">2. Related Work</h2>
<p><strong>channel attention(CA) :</strong> CAëŠ” high-level vision taskì—ì„œëŠ” ìì£¼ ì‚¬ìš©ë˜ì§€ë§Œ, low-level vision taskì—ì„œëŠ” ê±°ì˜ ì—°êµ¬ë˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ë•Œë¬¸ì—, ì €ìëŠ” SR Taskì˜ space limitationì— ë”°ë¼ì„œ, CNN-based methods / attention mechanismì— focus ë¥¼ ë§ì¶¥ë‹ˆë‹¤.<br /></p>

<p><strong>Predefined upsampling :</strong> í•´ë‹¹ ë°©ë²•ì€ LR â€“&gt; HR space ë¡œ mappingí•œ ë’¤ì—, í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ê³„ì‚°ëŸ‰ì— ëŒ€í•œ issueë„ ìˆì„ ë¿ë”ëŸ¬, interpolationìœ¼ë¡œ ì¸í•´ ì–»ëŠ” íš¨ê³¼ëŠ” resolutionì´ ì¦ê°€í•˜ëŠ”ê²ƒ ë°–ì— ì—†ìœ¼ë¯€ë¡œ ìµœê·¼ì—ëŠ” í•´ë‹¹ ë°©ë²•ì€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì €ìë„ ê°™ì€ ì´ì•¼ê¸°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p><strong>Thinking about Deep Network :</strong> ì €ìëŠ” EDSR/MDSRì„ ì˜ˆì‹œë¡œ ì„¤ëª…í•˜ê³  ìˆê³ , ë¬¼ë¡  ê¸°ì¡´ì˜ SRResNetêµ¬ì¡°ì—ì„œ ë¶ˆí•„ìš”í•œ ëª¨ë“ˆ(BN/ReLU)ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥í–¥ìƒì„ ì´ë£¨ì—ˆì§€ë§Œ, EDSR/MDSRì€ ë‹¤ë¥¸ networkì— ë¹„í•´ì„œ parameterìˆ˜ê°€ êµ‰ì¥íˆ ë§ì€ í¸ì´ë©°, residual blockì„ ê¹Šê²Œë§Œ ìŒ“ëŠ” ê²ƒì€ ìœ ì˜ë¯¸í•œ ë°©ë²•ì´ ì•„ë‹ˆë‹¤.ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ, Introducionì—ì„œ ë§í•œ, Issue 02ì— ëŒ€í•´ì„œ í•œë²ˆë” ë¶€ì •ì ì¸ ê²¬í•´ë¥¼ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h3 id="-attention-mechansim-">[ Attention mechansim ]</h3>
<p>ì¼ë°˜ì ìœ¼ë¡œ, Attention mechansimì€ ì„±ëŠ¥ì´ í–¥ìƒí•˜ëŠ” ìª½ìœ¼ë¡œ, biasë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
ìœ„ì—ë„ ë§í–ˆë“¯ì´ ì €ìëŠ” Attention mechansimì´ SR Taskì— ì„ í–‰ì—°êµ¬ë˜ì§€ ì•Šì•˜ê³  ì´ë¥¼ ì ìš©í•´ì„œ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥í–¥ìƒì„ ì´ëŒì–´ë‚¸ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ, high-frequency featureê°€ HR reconstructionì— ìœ ìš©í•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¹ì—°í•œ ê²ƒì…ë‹ˆë‹¤. ì˜ë¬¸ì„ ê°€ì§ˆ ì—¬ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ ì €ìëŠ” networkê°€ ì´ëŸ¬í•œ(useful feature)ì— ë” attentioní•œë‹¤ë©´ ì¢‹ì€ ê°œì„ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="3-residual-channel-attention-network-rcan">3. Residual Channel Attention Network (RCAN)</h2>

<h3 id="-network-architecture-">[ Network Architecture ]</h3>
<p><img src="../assets/img/RCAN.png" alt="Alt text" /><br /></p>
<h4 id="ilr--input">I<sub>LR</sub> = input</h4>
<h4 id="isr--output">I<sub>SR</sub> = output</h4>
<p><img src="../assets/img/RCAN/RCAN_02.png" alt="Alt text" /><br /></p>
<h4 id="hsf--feature-extraction">H<sub>SF</sub> = feature extraction</h4>
<p><img src="../assets/img/RCAN/RCAN_03.png" alt="Alt text" /><br /></p>
<h4 id="f0--deep-feature-extrationì—-ì‚¬ìš©ë©ë‹ˆë‹¤-rir-modeul-ì˜-inputì´-ë©ë‹ˆë‹¤">F<sub>0</sub> = deep feature extrationì— ì‚¬ìš©ë©ë‹ˆë‹¤. RIR modeul ì˜ inputì´ ë©ë‹ˆë‹¤.</h4>
<p><img src="../assets/img/RCAN/RCAN_04.png" alt="Alt text" /><br /></p>
<h4 id="fdf--deep-featureë¡œ-ê°„ì£¼í•˜ê³ -ì´ëŠ”-upscale-module-ì—-input-ì…ë‹ˆë‹¤">F<sub>DF</sub> = deep featureë¡œ ê°„ì£¼í•˜ê³ , ì´ëŠ” upscale module ì— input ì…ë‹ˆë‹¤.</h4>
<p><img src="../assets/img/RCAN/RCAN_05.png" alt="Alt text" /><br /></p>
<h4 id="hrec--upscaling--convupscaling--network-architectureì™€-ê°™ì´-ë³´ì‹œë©´-ì‰½ê²Œ-ë”°ë¼ê°ˆ-ìˆ˜-ìˆìŠµë‹ˆë‹¤">H<sub>REC</sub> = Upscaling â€“&gt; Conv(Upscaling),  Network architectureì™€ ê°™ì´ ë³´ì‹œë©´ ì‰½ê²Œ ë”°ë¼ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</h4>

<h3 id="-loss-function-">[ Loss-function ]</h3>
<p><img src="../assets/img/RCAN/RCAN_07.png" alt="Alt text" /><br /></p>

<p>ì €ìëŠ”, ì´ì „ì˜ ë…¼ë¬¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ RCANì˜ íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•˜ì—¬ ì´ì „ì˜ ì—°êµ¬ë“¤ì—ì„œ ì‚¬ìš©í•œ ë°©ì‹ì¸ L1-lossë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-residual-in-residual-rir-">[ Residual in Residual (RIR) ]</h3>
<p><img src="../assets/img/RCAN/RCAN_12.png" alt="Alt text" /><br /></p>

<p>EDSRì—ì„œ residual block, skip connectionì„ ì´ìš©í•˜ì—¬ Deep CNNì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì…ì¦ë¬ë‹¤ê³  í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, residual êµ¬ì¡°ë¡œ ì¸í•˜ì—¬ 400ê°œ ì´ìƒì˜ ë§ì€ layerë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.<br />
g-th group ìœ¼ë¡œ ì´ë£¨ì–´ì§„, RGëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™” ëœë‹¤ê³  í•©ë‹ˆë‹¤.<br />
<img src="../assets/img/RCAN/RCAN_09.png" alt="Alt text" /><br /></p>
<h4 id="hg--g-th-rg">H<sub>g</sub> = g-th RG</h4>
<h4 id="fg-1--input-for-g-th-rg">F<sub>g-1</sub> = input for g-th RG</h4>
<h4 id="fg--output-for-g-th-rg">F<sub>g</sub> = output for g-th RG</h4>
<p>ì €ìëŠ” simple í•˜ê²Œ RG ë§Œ ë§ì´ ìŒ“ëŠ”ë‹¤ê³ í•´ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒì€ í™•ì¸í•˜ì§€ ëª»í•˜ì˜€ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ LSCë¥¼ RIRì— ì¶”ê°€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, trainingì„ ì•ˆì •í™”í•˜ë©´ì„œ ì„±ëŠ¥í–¥ìƒì„ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="../assets/img/RCAN/RCAN_10.png" alt="Alt text" /><br /></p>
<h4 id="wlsc--rir-conv-layer-weight-tail--biasx">W<sub>LSC</sub> = RIR conv layer weight (tail) , bias(x)</h4>

<p>ê²°ê³¼ì ìœ¼ë¡œ, RCAB(b-th residual, g-th RG)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™” ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<img src="../assets/img/RCAN/RCAN_11.png" alt="Alt text" /><br /></p>
<h4 id="fgb-1--g-th-rg-ì—ì„œ-b-th-rcab-input">F<sub>g,b-1</sub> = g-th RG ì—ì„œ b-th RCAB input</h4>
<h4 id="fgb--g-th-rg-ì—ì„œ-b-th-rcab-output">F<sub>g,b</sub> = g-th RG ì—ì„œ b-th RCAB output</h4>
<p>ì €ìëŠ” LR input / fetures ì—ëŠ” abundant informationì´ ë§ê³ , SR network ì˜ ëª©í‘œëŠ” useful informationì„ recover í•˜ëŠ” ê²ƒì´ë‹¤. ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. abundant information(low-frequency)ëŠ” identity-based skip connectionìœ¼ë¡œ ì „ë‹¬ ëœë‹¤ê³ í•©ë‹ˆë‹¤. residual lerningì„ ìœ„í•´ì„œ B residual channel attention blockì„ ìŒ“ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ë•Œ, B residual channel attention blockì€ ê°ê°ì˜ RGì— í¬í•¨ëœë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p><img src="../assets/img/RCAN/RCAN_13.png" alt="Alt text" /><br /></p>

<h3 id="-channel-attentionca-">[ Channel Attention(CA) ]</h3>

<p><img src="../assets/img/RCAN/RCAN_14.png" alt="Alt text" /><br /></p>

<p>ì €ìëŠ” feature map ì—ì„œ channel ê°„ì— ìƒí˜¸ì˜ì¡´ì„±ì„ ì´ìš©í•˜ì—¬ CAë¥¼ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.<br />
ê° channel ë³„ë¡œ different attentioní•˜ëŠ” ê²ƒì´ key step ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë•Œ, ë‘ê°€ì§€ concernsê°€ ìˆìŠµë‹ˆë‹¤.<br /></p>

<p><strong>First :</strong> LR space ëŠ” ë‹¹ì—°íˆ abundant low-frequency information, valuable high-frequency componetsë¥¼ ê°–ìŠµë‹ˆë‹¤. low-frequency partëŠ” more complanteí•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  high-frequency componetsëŠ”  ì¼ë°˜ì ìœ¼ë¡œ, edge, texture, detail informationì„ ê°–ìŠµë‹ˆë‹¤.<br />
<strong>Second :</strong>ê°ê°ì˜ Conv layer ëŠ” local receptive field ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, Conv ëª…ë ¹ì„ ìˆ˜í–‰í•œ ë’¤, outputì€ unable to exploit contextual inforamtion outside of the local region í•œ ê²ƒì€ ë‹¹ì—°í•œ ê²ƒ ì…ë‹ˆë‹¤.<br /></p>

<p>ì €ìëŠ” First, Second ì— ë¶„ì„ì— ê¸°ì´ˆí•˜ì—¬, global average pooling ì„ ì‚¬ìš©í•˜ì—¬ channel-wise global spatial informationì„ ê°€ì ¸ì˜¨ë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p>GAP (global average pooling)ì€ HxW pooling ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¦‰, í•œ feature map (HxWxC)ë¥¼ ì „ë¶€ pooling í•˜ì—¬ í•˜ë‚˜ì˜ neuron(1x1xC)ë¡œ mapping í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.
ì—¬ê¸°ì„œ GAP ì´í›„ì— softmaxë¥¼ ì·¨í•˜ë©´, ê° feature map ì˜ ì¤‘ìš”ë„(?)ê°€ ê³„ì‚°ë  ìˆ˜ ìˆì„ê²ƒì´ë¼ê³  ì˜ˆìƒì´ ë©ë‹ˆë‹¤.<br /></p>

<p><img src="../assets/img/RCAN/RCAN_15.png" alt="Alt text" /><br /></p>

<h4 id="x--x1-xc-xc-c--feature-map">X = [x1, â€¦,x<sub>c</sub>,â€¦ x<sub>C</sub>], C = feature map</h4>
<h4 id="hgp-gp--global-pooling">H<sub>GP</sub>, GP = Global pooling</h4>
<h4 id="zc-channel-statistic-local-descriptors">Z<sub>c</sub>, Channel statistic (local descriptors)</h4>
<p>ë” ì •êµí•œ ë°©ë²•ì´ ë„ì…ë  ìˆ˜ ìˆë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. <br /></p>

<p>ì´ì œ, GAPë¥¼ í†µí•´ì„œ aggregate ëœ ì •ë³´ë¡œ ë¶€í„° channel-sise dependencyë¥¼ ì•Œê¸° ìœ„í•´ì„œ Gating mechanismì„ ì†Œê°œí•œë‹¤ê³  í•©ë‹ˆë‹¤.<br />
Gating mechanismì„ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ì˜ ê¸°ì¤€ì´ ë§Œì¡±ë˜ì–´ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. <br />
<strong>First :</strong> channel ì‚¬ì´ì˜ nonlinear interactionì„ learningí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.<br />
<strong>Second :</strong> Ont-hot activation ê°™ì´ í•˜ë‚˜ë§Œ ê°•ì¡°í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ê°ê°ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.<br /></p>

<p>ì €ìëŠ”, First, Secondë¥¼ ë”°ë¼ì„œ, sigmoid functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
<img src="../assets/img/RCAN/RCAN_16.png" alt="Alt text" /><br /></p>

<h4 id="where-f--and-Î´--denote-the-sigmoid-gating-and-relu-34-function-respectively-wd-is-the-weight-set-of-a-conv-layer-which-acts-as-channel-downscaling-with-reduction-ratio-r-after-being-activated-by-relu-the-low-dimension-signal-is-then-increased-with-ratio-r-by-a-channel-upscaling-layer-whose-weight-set-is-wu--then-we-obtain-the-final-channel-statistics-s-which-is-used-to-rescale-the-input-xc-ì…ë‹ˆë‹¤ì›ë¬¸ë‚´ìš©-">where f (Â·) and Î´ (Â·) denote the sigmoid gating and ReLU [34] function, respectively. WD is the weight set of a Conv layer, which acts as channel-downscaling with reduction ratio r. After being activated by ReLU, the low-dimension signal is then increased with ratio r by a channel-upscaling layer, whose weight set is WU . Then we obtain the final channel statistics s, which is used to rescale the input xc ì…ë‹ˆë‹¤.(ì›ë¬¸ë‚´ìš©) <br /></h4>
<p>ê°„ë‹¨í•˜ê²Œ, input x<sub>c</sub>ì„ ë°›ì•„ì„œ, ReLUë¥¼ ì·¨í•œ ë’¤, W<sub>U</sub>ë§Œí¼, upscaling ì„ í•©ë‹ˆë‹¤. (weight scaling) ê·¸ ë’¤, Sigmoid functionì„ ì‚¬ìš©í•˜ì—¬, rescaling ëœ input x<sub>c</sub>ë¥¼ ì–»ì–´ëƒ…ë‹ˆë‹¤. rescaling ëœ  x<sub>c</sub> = s ì…ë‹ˆë‹¤.</p>
:ET