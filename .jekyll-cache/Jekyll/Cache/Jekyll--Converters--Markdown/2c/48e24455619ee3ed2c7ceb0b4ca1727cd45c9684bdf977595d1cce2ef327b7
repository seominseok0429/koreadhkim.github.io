I"n4<h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1807.02758.pdf (ECCV 2018)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>SR에서 network 의 deep할수록 좋다.! 라는 것은 VDSR을 통해서 많이 알려져 있습니다. 그러나, 기본적으로 network가 deep할수록 학습이 어려운것은 보편적인 사실입니다. 때문에, 이를 해결하기 위해서 많은 학습기법들이 제안되어 왔습니다. classification과 같은 task는 학습이 비교적 쉽다고 볼 수 있습니다. 그러나, SR task는 학습이 불안정하다.(까다롭다.)라고 볼 수 있습니다. 따라서, 저자는 SR task에서 network가 deep 할수록 train에 어려움을 겪는다고 말하고 있습니다. (제안된 네트워크 관련 논문들은 대부분 제안한 모델을 학습하기 위한 기법을 같이 제안하는식으로 알고 있습니다.) 따라서, 논문은 이러한 문제점을 해결하기 위해서 RCAN을 제안한다고 합니다. very deep network를 구성하기 위해서 Residual in Residual (RIR)을 제안 한다고 합니다. RIR은 몇개의 residual groups으로 구성되며 long skip connection을 갖고 있다고 합니다.
저자는 또한, low-resolution image input 과 feature 에는 channel별로 동등하게 여겨지는 abundant low-frequency information을 포함하고 있기 때문에 CNN 구조의 장점인 representational ability 를 제한한다고 말하고 있습니다.
논문에서는 channel 사이 상호의존성을 고려하여 channel attention mechanism을 제안합니다.<br /></p>

<p>CNN에서 말하는 channel attention mechanism을 쉽게 이해하고 가려면 다음그림을 보시면 될 것 같습니다.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>다수의 SR논문에서 (eg.SISR) Introduction에서 말하는 내용은 비슷합니다.(제가 읽은 SISR에 한해서)<br />
(1) LR –&gt; HR Task이다.<br />
(2) ill-posed problem이다.<br />
(3) 이러한 issue 를 해결하기 위해 많은 방법이 제안되었다.<br />
(4) (3)에서 말한 많은 방법들에 대한 맛보기.<br />
(5) (4)에서 말한 맛보기에서 issue들을 나열<br /></p>

<hr />

<p>많은 분들이 아시다시피, residual을 의미없이 많이 삽입하고, deep network를 구축한다고 해서 성능이 유의미하게 좋아지는 것은 아닙니다. 정교한 residual이 성능을 유의미하게 향상시킬것입니다. 저자도 유사한 이야기를 하고 있습니다.</p>

<p><strong>issue 01 :</strong> residual 을 의미없이 많이 삽입하고, Deep network를 구축하는 방식<br />
<strong>issue 01-related problem:</strong><br />
(1) parameter increment / a fall in computational efficiency.
(2) (1)에도 불구하고, 성능향상폭은 낮음.</p>

<p><strong>issue 02 :</strong> [1~11] 에서 제안된 논문들은 channelwise features 를 동등하게 다루고 있다.<br />
<strong>issue 02-related problem:</strong><br />
(1) low/high frequency information이 서로 dealing 하는데 있어서 flexibility가 결여된다.<br />
(2) wastes unnecessary computations(low-frequency 관점)<br />
(3) discriminative ability leanring X (channel 관점) –&gt; deep network의 representational power 를 낮춤.</p>

<p><strong>Therefore, proposed RCAN :</strong> residual을 의미있게 가져가면서, channelwise features를 차별성을 두고 다루겠다. 풀어쓰면, 아래와 같습니다.<br />
LR –&gt; Network –&gt; HR 에서, LR은 High-frequency information 보다 low-frequnecy information이 상대적으로 많이 포함되어 있습니다. 그러면, Network는 결국 LR에서 부족한 정보인 high-frequency inforamtion을 restore하는 방향으로 학습될 것 입니다. 그러나, 이때, LR에 포함된 Low-frequency information 도 잃어버리지 않기 위해 residual을 어떻게 쓰느냐에 대한 연구도 많이 있습니다. 저자는 깊은 network에서 low-frequency inforamtion을 compute하는데 많은 낭비를 한다고 말하고 있으며, feature channels을 discriminative 하게 learning 하는 능력이 부족하다고 생각하고 있습니다. 결과적으로, 이러한 문제점들은 HR(output)이 representation하는데 악영향을 미칠 수 있다. 라고 말하고 있습니다.<br /></p>

<p><strong>Approximate RCAN information :</strong> RIR(RG(SSC),LSC)<br />
풀어쓰면 아래와 같습니다. RIR(residual in residual)구조를 제안하며, RIR은 residual group(RG)와 long-skip-connection(LSC)를 포함하고 있다고 합니다. 이때, RG는 short-skip-connection(SSC)를 사용하여, EDSR의 residual block을 쌓는다고 합니다. LSC, SSC가 포함됨으로인해 low-frequency information을 잘 bypass할 수있으며, information flow를 원활하게 가져간다고 하고있습니다.<br /></p>

<p>또한, Issue 02 를 해결하기 위하여 저자는 channel attention(CA) mechanism을 제안합니다. 이는 직관적으로 정답에 근사하기위해, Loss를 줄이기 위해 useful feature를 고르겠다는 것으로 보면 될 것 같습니다.<br /></p>

<p><strong>Overall, our contributions are three-fold:</strong><br />
<strong><u>[1]</u></strong> : 성능 높은 SR을 위하여 RCAN을 제안하며 이는 이전의 제안된 방법들보다 좋은 성능을 보입니다.<br />
<strong><u>[2]</u></strong> : Deep-Network training이 가능한 RIR구조를 제안합니다. RIR내부에 존재하는 SSC/LSC는 network가 low-frequency information을 잘 전달하며, network가 더 효과적인 정보를 training할 수 있도록 도와줍니다.<br />
<strong><u>[3]</u></strong> : CA(channel Atteation) mechanism을 제안하고 CA로 인하여 network의 representational ability 를 향상 시킵니다. <br /></p>

<p>정성적인 성과는 다음과 같다고 말하고 있습니다.<br />
<img src="../assets/img/RCAN/RCAN_01.png" alt="Alt text" /><br /></p>

<h2 id="2-related-work">2. Related Work</h2>
<p><strong>channel attention(CA) :</strong> CA는 high-level vision task에서는 자주 사용되지만, low-level vision task에서는 거의 연구되지 않았다고 합니다. 때문에, 저자는 SR Task의 space limitation에 따라서, CNN-based methods / attention mechanism에 focus 를 맞춥니다.<br /></p>

<p><strong>Predefined upsampling :</strong> 해당 방법은 LR –&gt; HR space 로 mapping한 뒤에, 학습하는 방법입니다. 이는 계산량에 대한 issue도 있을 뿐더러, interpolation으로 인해 얻는 효과는 resolution이 증가하는것 밖에 없으므로 최근에는 해당 방법은 사용되지 않는 것으로 알고 있습니다. 저자도 같은 이야기를 하고 있습니다.</p>

<p><strong>Thinking about Deep Network :</strong> 저자는 EDSR/MDSR을 예시로 설명하고 있고, 물론 기존의 SRResNet구조에서 불필요한 모듈(BN/ReLU)를 제거함으로써 성능향상을 이루었지만, EDSR/MDSR은 다른 network에 비해서 parameter수가 굉장히 많은 편이며, residual block을 깊게만 쌓는 것은 유의미한 방법이 아니다.라고 말하고 있습니다. 그러면서, Introducion에서 말한, Issue 02에 대해서 한번더 부정적인 견해를 말하고 있습니다.</p>

<h3 id="-attention-mechansim-">[ Attention mechansim ]</h3>
<p>일반적으로, Attention mechansim은 성능이 향상하는 쪽으로, bias되는 방향으로 학습하는 방법이라고 생각할 수 있을 것 같습니다.
위에도 말했듯이 저자는 Attention mechansim이 SR Task에 선행연구되지 않았고 이를 적용해서 유의미한 성능향상을 이끌어낸것으로 보입니다. 그러면서, high-frequency feature가 HR reconstruction에 유용하다고 합니다. 이는 당연한 것입니다. 의문을 가질 여지가 없습니다. 그러면서 저자는 network가 이러한(useful feature)에 더 attention한다면 좋은 개선효과를 얻을 수 있을 것이라고 생각하고 있습니다.</p>

<h2 id="3-residual-channel-attention-network-rcan">3. Residual Channel Attention Network (RCAN)</h2>

<h3 id="-network-architecture-">[ Network Architecture ]</h3>
<p><img src="../assets/img/RCAN.png" alt="Alt text" /><br /></p>
<h4 id="ilr--input">I<sub>LR</sub> = input</h4>
<h4 id="isr--output">I<sub>SR</sub> = output</h4>
<p><img src="../assets/img/RCAN/RCAN_02.png" alt="Alt text" /><br /></p>
<h4 id="hsf--feature-extraction">H<sub>SF</sub> = feature extraction</h4>
<p><img src="../assets/img/RCAN/RCAN_03.png" alt="Alt text" /><br /></p>
<h4 id="f0--deep-feature-extration에-사용됩니다-rir-modeul-의-input이-됩니다">F<sub>0</sub> = deep feature extration에 사용됩니다. RIR modeul 의 input이 됩니다.</h4>
<p><img src="../assets/img/RCAN/RCAN_04.png" alt="Alt text" /><br /></p>
<h4 id="fdf--deep-feature로-간주하고-이는-upscale-module-에-input-입니다">F<sub>DF</sub> = deep feature로 간주하고, 이는 upscale module 에 input 입니다.</h4>
<p><img src="../assets/img/RCAN/RCAN_05.png" alt="Alt text" /><br /></p>
<h4 id="hrec--upscaling--convupscaling--network-architecture와-같이-보시면-쉽게-따라갈-수-있습니다">H<sub>REC</sub> = Upscaling –&gt; Conv(Upscaling),  Network architecture와 같이 보시면 쉽게 따라갈 수 있습니다.</h4>

<h3 id="-loss-function-">[ Loss-function ]</h3>
<p><img src="../assets/img/RCAN/RCAN_07.png" alt="Alt text" /><br /></p>

<p>저자는, 이전의 논문들과 비교하여 RCAN의 효과를 보여주기 위하여 이전의 연구들에서 사용한 방식인 L1-loss를 사용하였다고 합니다.</p>

<h3 id="-residual-in-residual-rir-">[ Residual in Residual (RIR) ]</h3>
<p><img src="../assets/img/RCAN/RCAN_12.png" alt="Alt text" /><br /></p>

<p>EDSR에서 residual block, skip connection을 이용하여 Deep CNN을 설계할 수 있다는 것이 입증됬다고 합니다. 결과적으로, residual 구조로 인하여 400개 이상의 많은 layer를 학습할 수 있게 됩니다.<br />
g-th group 으로 이루어진, RG는 다음과 같이 공식화 된다고 합니다.<br />
<img src="../assets/img/RCAN/RCAN_09.png" alt="Alt text" /><br /></p>
<h4 id="hg--g-th-rg">H<sub>g</sub> = g-th RG</h4>
<h4 id="fg-1--input-for-g-th-rg">F<sub>g-1</sub> = input for g-th RG</h4>
<h4 id="fg--output-for-g-th-rg">F<sub>g</sub> = output for g-th RG</h4>
<p>저자는 simple 하게 RG 만 많이 쌓는다고해서 성능이 좋아지는 것은 확인하지 못하였고, 이를 해결하기 위하여 LSC를 RIR에 추가로 사용합니다. 결과적으로, training을 안정화하면서 성능향상을 이끌어 낼 수 있었다고 합니다.</p>

<p><img src="../assets/img/RCAN/RCAN_10.png" alt="Alt text" /><br /></p>
<h4 id="wlsc--rir-conv-layer-weight-tail--biasx">W<sub>LSC</sub> = RIR conv layer weight (tail) , bias(x)</h4>

<p>결과적으로, RCAB(b-th residual, g-th RG)는 다음과 같이 공식화 될 수 있습니다.
<img src="../assets/img/RCAN/RCAN_11.png" alt="Alt text" /><br /></p>
<h4 id="fgb-1--g-th-rg-에서-b-th-rcab-input">F<sub>g,b-1</sub> = g-th RG 에서 b-th RCAB input</h4>
<h4 id="fgb--g-th-rg-에서-b-th-rcab-output">F<sub>g,b</sub> = g-th RG 에서 b-th RCAB output</h4>
<p>저자는 LR input / fetures 에는 abundant information이 많고, SR network 의 목표는 useful information을 recover 하는 것이다. 라고 말하고 있습니다. abundant information(low-frequency)는 identity-based skip connection으로 전달 된다고합니다. residual lerning을 위해서 B residual channel attention block을 쌓는다고 합니다. 이때, B residual channel attention block은 각각의 RG에 포함된다고 합니다.<br /></p>

<p><img src="../assets/img/RCAN/RCAN_13.png" alt="Alt text" /><br /></p>

<h3 id="-channel-attentionca-">[ Channel Attention(CA) ]</h3>

<p><img src="../assets/img/RCAN/RCAN_14.png" alt="Alt text" /><br /></p>

<p>저자는 feature map 에서 channel 간에 상호의존성을 이용하여 CA를 제안하였습니다.<br />
각 channel 별로 different attention하는 것이 key step 이라고 합니다. 이때, 두가지 concerns가 있습니다.<br /></p>

<p><strong>First :</strong> LR space 는 당연히 abundant low-frequency information, valuable high-frequency componets를 갖습니다. low-frequency part는 more complante하다고 합니다. 그리고 high-frequency componets는  일반적으로, edge, texture, detail information을 갖습니다.<br />
<strong>Secound :</strong>각각의 Conv layer 는 local receptive field 를 포함합니다. 따라서, Conv 명령을 수행한 뒤, output은 unable to exploit contextual inforamtion outside of the local region 한 것은 당연한 것 입니다.<br /></p>

<p>저자는 First, Secound 에 분석에 기초하여, global average pooling 을 사용하여 channel-wise global spatial information을 가져온다고 합니다.<br /></p>

<p>GAP (global average pooling)은 HxW pooling 이라고 생각하시면 될 것 같습니다. 즉, 한 feature map (HxWxC)를 전부 pooling 하여 하나의 neuron(1x1xC)로 mapping 하는 것 입니다.</p>
:ET