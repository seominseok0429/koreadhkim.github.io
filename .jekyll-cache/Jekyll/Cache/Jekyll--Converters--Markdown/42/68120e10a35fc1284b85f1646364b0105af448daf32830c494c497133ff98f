I".!<h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1803.02735 (CVPR 2018)</p>

<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>Feed-forward architecture는 저해상도와 고해상도 이미지의 상호의존 관계를 완전히 설명하지 못한다고 합니다. 
여기서 말하는 저해상도 이미지와 고해상도 이미지의 상호의존관계는 feed-forward 구조의 경우 어쨋든 input은 LR image 이며 모든 layer들이 LR image에서 extraction된 정보만 이용하기 때문에, LR image &lt;&gt; HR image 사이의 의존관계를 부정적으로 바라보고 있는 것 같습니다. 즉, 어느순간에는 LR image으 정보로만 HR image를 만들어 내는 것은 부적절 하다. 까지 바라볼 수 있었습니다.</p>

<blockquote> We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and downsampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually connected up- and down-sampling stages each of which represents different types of image degradation and highresolution components.
</blockquote>

<p>눈길을 끄는 단어는 *feedback mechanism, *projection error, *image degradation, *high-resolution componets. 입니다.</p>

<p>저자의 말에 따르면, 제안하고자 하는 네트워크에는 반복적인 Up/Down sampling layer를 포함하고, Up/Down sampling layer 들은 projection error를 계산하고 이를 공유하여 성능을 보이겠다. 라고 하는것으로 보입니다. 이를 (feedback mechanism) 이라고 하는 것 같습니다.</p>

<p>network에서 반복되는 Up/Down sampling 을 각각 output_Up(Upsample), output_Down(Downsample)이라고 하면, output_UP를 HR중 하나로 보고, output_Down를 LR중 하나로 보고 있다고 생각이 됩니다.</p>

<p>핵심은, 이를 통해서 x8 (lager scale factor) 에서 성능을 보여줬다는 것입니다.</p>

<p>** 당시 Challenge (x8) track 에서 1위를 하였습니다. **</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>저자는 계속적으로 이전방식들의 메커니즘을 설명하고 있습니다. 이는 “ non-linear LR-to-HR mapping “으로 간단히 말할 수 있을 것 입니다.
[6,7,37,24,21,22,42] 에 해당하는 네트워크 구조들은 LR image 에서 feature extraction을 한 뒤, 1 or N개의 upsampling layer 를 통해 HR space 로 확장합니다.
이러한 방식을 Feed-Forward 방식 입니다.
저자는 human visual system은 feedback connection을 사용하는 것으로 파악하고 있다고 합니다. (이를 SR 네트워크에 적용하겠다.)
따라서, Feed-Back이 없기 때문에, 기존에 Feed-Forward connection만 존재하는 SR 구조들은 large scale 에서 LR –&gt; HR 어려움을 겪는다고 합니다.</p>

<p><strong><u>Error feedback</u></strong> : iterative error correcting geedback mechanism 을 SR에서 제안한다고 합니다. up/down projection(sampling)의 error를 통해 reconstruction 결과를 더 좋게 Guide 한다고 합니다. 여기서 중요한 것은 이러한 방법이 어떤 역할을 하는가? 일텐데, 저자는 projection error를 사용함으로써 early layers 를 characterize or constraint 한다고 합니다.</p>

<p><strong><u>Mutually connected up- and down-sampling stages</u></strong> : LR image 의 representation 만 HR space 로 mapping 합니다. (oneway mapping) 당연히, 해당 방법의 경우 large scale factor에 대해서 강인하기 힘든 것이 사실입니다. input 즉, LR에서의 정보량은 제한되어 있기때문에 정보량 관점에서 large scale factor로 mapping하려면 디테일한 정보들을 살려야 할텐데, 필요한 정보량 대비 갖는 정보량이 매우 적다고 판단됩니다.</p>

<p>따라서, 해당 논문에서는 upsampling 을 사용하여, 가능한 HR feature를 다양하게 만드는 것에 초점을 맞추고, 더불어서, 만들어진 HR feature를 downsampling 을 통하여, 다시 LR space 로 projection 한다고 합니다.</p>

<p>SR은 일반적으로 1 : N 의 답이 나올 수 있는 Task입니다.(ill-posed problom), 1(LR) : N(HR) OR 1(HR) : N(LR), 두 경우 모두 포함된다고 생각합니다. 논문에서는 두 가지 경우를 모두 고려하는 것으로 보여집니다. 여러개의 HR image 로  여러개의 LR image를 만들고 결과적으로 초록에 쓰여있던것 처럼 각각 다른 HR 구성요소와, image degration을 표현이 가능하게 하는 것으로 보여집니다.
다음의 그림들은 기존 제안된 방식과, 본 논문에서 제안하는 방식입니다.</p>

<p><strong><u>Deep concatenation</u></strong> : 직관적으로 보이는 network의 이점은 다양한 type의 image degradation 과 HR componete 를 나타내는 것입니다. 이때, 논문에서는 모든 output(Upsampling) 을 concat하여 HR image로 복원한다고 합니다. Figure2(d)의 빨간선을 보시면 됩니다.</p>

<p><strong><u>Improvement with dense connection</u></strong> : 해당 논문의 아이디어에 dense connection을 추가하여 성능을 향상시켰다고 합니다.</p>

<h2 id="2-relatedwork">2. Relatedwork</h2>
<p><img src="../assets/img/DBPN/DBPN_02.png" alt="Alt text" /></p>

<h3 id="a-predefined-upsampling-egsrcnn-vdsr-drrn">(a) Predefined upsampling (eg.SRCNN, VDSR, DRRN)</h3>
<p>해당하는 구조는 LR image를 interation을 통해, HR space로 변환 후 복원하는 방식입니다. 앞단에서 interpolation을 사용하여 middel resolution(MR)을 생성한다고 합니다. SRCNN의 방식과 같습니다. 논문에서도 말하고 있지만, 이러한 방식은 MR image 에 새로운 노이즈를 발생시킬 수 있는 단점이 있습니다.</p>

<h3 id="b-single-upsampling-egfsrcnn-espcn-edsr">(b) Single upsampling (eg.FSRCNN, ESPCN, EDSR)</h3>
<p>이러한 방식은 spatial resolution 을 높이고 (a)에서 앞단에서 사용한 interpolation과 같은 predefined operators 를 대체 가능합니다. 논문에서는 이러한 방법에서 Network capacity 가 제한되어 있어 복잡한 mapping은 학습에 실패한다고 합니다. EDSR network 가 NTIRE2017에서 우승하였지만, 매우 많은 파라미터가 요구됩니다.</p>

<h3 id="c-progressive-upsampling-eglapsrn">(c) Progressive upsampling (eg.LapSRN)</h3>
<p>이러한 방식은 LapSRN 에서 처음 제안되었습니다. network에서 서로 다른 scale로 upsampling합니다. 그러나, 단순하게 해당 네트워크는 limited LR features에 의존하는 single upsampling stack이라고 볼 수 있다고 합니다. 결과적으로 LapSRN은 lager scale factor x8 에서 shallow network로 좋은 성능을 보입니다.</p>

<h3 id="d-iterative-up-and-downsampling-proposed">(d) Iterative up and downsampling (proposed)</h3>
<p>논문에서 제안하는 SR network 입니다. 저자는 different depth(different layer)와 distribute 에서 SR feature의 sampling rate를 증가시키는 것에 초점을 맞춘다고 합닝다. (각 stage 마다  reconstruction error를 계산)
해당 schema는 네트워크가 보다 deep feature를 생성할 수있게 하면서 upsampling 을 학습하여 HR component를 보존한다고 합니다.</p>

<h3 id="back-projection">Back-projection</h3>
<p>[18] 의 Back-projection은 reconstruction error 를 minimize 하는 efficient iterative procedure로 알려져 있습니다. 여러 연구들에서 back projection이 유의미하다는 것을 입증하였슨비다.
originally back-porjection은 Multi LR input 이 있는 경우 적합하게 designe 되었다고 합니다. single LR input이라면, updating procedure은 multiple upsampling operator를 사용하고 reconstruction error 를 반복적으로 계산함으로써 가능하다고 합니다. 이와 비슷하게 SR Task 에 적용한 연구들이 있었습니다.
이전 연구들을 확장하여 해당 논문은 SR에서 architecture를 제안하였습니다.</p>

<h2 id="3-deep-back-projection-networks">3. Deep Back-Projection Networks</h2>

<p>논문에서 제안하는 Projection unit의 구성을 살펴 보겠습니다.<br />
<img src="../assets/img/DBPN/DBPN_05.png" alt="Alt text" /></p>

:ET