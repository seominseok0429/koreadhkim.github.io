I"Ô;<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>https://arxiv.org/pdf/1903.09814 (CVPR 2019)</p>

<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>DBPNì—ì„œ ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì €ìëŠ” ê¸°ì¡´ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì€ HVS(human visual system)ì— ì¡´ì¬í•˜ëŠ” feedback mechanismì„ ì™„ì „íˆ í™œìš©í•˜ì§€ ëª»í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” low-level representationì„ high-level informationì„ ì´ìš©í•˜ì—¬ ê°œì„ í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë–„, feedback ë°©ì‹ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•˜ì—¬ hidden states (RNN)ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. feedback block ì€ feedback connectionì„ handeling í•  ìˆ˜ ìˆë„ë¡ designe ë˜ì—ˆê³ , ì´ëŠ” powerful high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ SRBFNì€ strong early reconstruction abilityë¥¼ ê°–ê³  ìˆìœ¼ë©°, step by step ìœ¼ë¡œ ìµœì¢… HR image ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” SR task ì—ì„œ curriculum learning ë°©ë²•ì„ í•™ìŠµì „ëµìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ì–´ë–»ê²Œ ì‚¬ìš©í•˜ì˜€ì§€ë„ êµ‰ì¥í•œ ê¶ê¸ˆì¦ì„ ìœ ë°œí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ì €ìëŠ” multiple type degradation taskì¸ SRì— ì í•©í•˜ë‹¤ê³  íŒë‹¨ í•˜ì˜€ìŠµë‹ˆë‹¤.
ë‹¤ìŒì˜ ê·¸ë¦¼ì€ RNN(Recurrent Neural Network)ì˜ hidden state ì…ë‹ˆë‹¤.<br />
<img src="/assets/img/SRFBN/SRFBN_02.png" alt="RNN hidden state" />
RNN ì˜ ê¸°ë³¸êµ¬ì¡°ì…ë‹ˆë‹¤. Green box = hidden state, Red box = input, Blue box = output ì…ë‹ˆë‹¤.
ì§ê´€ì ìœ¼ë¡œ ë³´ë©´, Green boxëŠ” inputê³¼ ë”ë¶ˆì–´ ì´ì „ term ì˜ Green boxë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ New Blue box(output)ì„ ìƒì„±í•œê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. CNNì—ì„œ ìƒê°í•´ë³´ë©´, layer ë¥¼ F(first) M(middel) F(final) ì´ë¼ê³  ìƒê°í•˜ë©´, Mt input = Ft + Mt-1 ì´ë¼ê³  ìƒê° í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>image super - resolution (SR) ì€ low-level computer vision task ì…ë‹ˆë‹¤. ëª¨ë“  ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ê²ƒ ì²˜ëŸ¼, SRì€ ì¼ë°˜ì ìœ¼ë¡œ 1 : N ì˜ ë‹µì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” Taskì…ë‹ˆë‹¤.(ill-posed problom), 1(LR) : N(HR) OR 1(HR) : N(LR), ë‘ ê²½ìš° ëª¨ë‘ í¬í•¨ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. 
ì´ì „ì— ì–´ë–¤ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì´ ìˆì—ˆëŠ”ì§€ ëŒ€ëµì ì¸ ë¶€ë¶„ì€ DBPN paper ì— ì˜ ì •ë¦¬ê°€ ë˜ì–´ìˆìŠµë‹ˆë‹¤.</p>

<p>ë‹¹ì—°íˆ network êµ¬ì¡°ê°€ ê¹Šì–´ì§ì— ë”°ë¼ parameterê°€ ì¦ê°€í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” overfitting ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.
parameterë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ recurrent structureë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. SR task ë„ ë§ˆì°¬ê°€ì§€ ì…ë‹ˆë‹¤. (eg.DRCN, DRRN) ì´ëŸ¬í•œ êµ¬ì¡°ì—ì„œ single-state Recurrent nuiral network(RNN)ìœ¼ë¡œ ì¶”ë¡ í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ deep learning based methodì™€ ìœ ì‚¬í•˜ê²Œ recurrent structure networkë„ feed-forwoar ë˜ë©´ì„œ ì •ë³´ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ feed-forward ë°©ì‹ì€ skip connectionì„ ì‚¬ìš©í•˜ë”ë¼ë„ previous layerì—ì„œ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” layer ì˜ useful informationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë„ ë‹¹ì—°í•œ ê²ƒì´ ì¼ë²ˆì ìœ¼ë¡œ F(First) M(middel) F(final) ì˜ layer ê°€ ìˆìœ¼ë©´ ì¼ë°˜ì ì¸ feed-forward êµ¬ì¡°ì—ì„œëŠ” F(first)ëŠ” inputì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  M(middel)ì€ Fout(first output)ì´ ì…ë ¥ì´ë¯€ë¡œ F(first)ëŠ” Mout(middle output)ì„ ì ‘ê·¼ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<p>ì €ìëŠ” congnition theory ì—ì„œ feedback connectionì´ ê³ ì°¨ì› ì˜ì—­ìœ¼ë¡œ ì €ì°¨ì›ì˜ì—­ìœ¼ë¡œ ì‘ë‹µ ì‹ í˜¸ë¥¼ ì „ì†¡í•œë‹¤.ë¼ê³  í•˜ëŠ”ë°, ì‰½ê²Œë§í•´ì„œ, high-level feature ë¥¼ low-level layerì— ì „ë‹¬ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤. DBPNì—ì„œëŠ” Up/Down sampling Unit ì„ í†µí•˜ì—¬ feedback mechanismì„ ì ìš©í•˜ì˜€ëŠ”ë°, ì´ëŠ” ê°„ë‹¨í•˜ê²Œ Upsampling featureë¥¼ ë‹¤ì‹œ Down sampling í•˜ì—¬ low-level ë¡œ projectioní•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” feed-back connectionì„ í†µí•˜ì—¬ high-level informationì„ low-level informationì„ refine í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ feed-back blockì„ ë³´ìœ í•˜ê³  ìˆëŠ” RNN êµ¬ì¡°ë¼ê³  í•©ë‹ˆë‹¤. í•´ë‹¹ network ëŠ” dense skip connectionì´ ìˆëŠ” ì—¬ëŸ¬ê°œì˜ Up/Down sampling ì„ í†µí•˜ì—¬ strong high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>[40]ì—ì„œ ì˜ê°ì„ ë°›ì•„ì„œ FBì˜ outputì— hidden state in an unfolded RNNì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤ (achieve the feedback manner)</p>

<p><img src="../assets/img/SRFBN/SRFBN_03.png" alt="Alt text" />
<img src="../assets/img/SRFBN/SRFBN_04.png" alt="Alt text" /></p>

<p>ìœ„ì˜ ê·¸ë¦¼ì„ ì„¸ë¡œë¡œ ì„¸ìš°ë©´ ìœ„ì—ì„œ ì–¸ê¸‰í•œ RNN êµ¬ì¡°ì™€ ë˜‘ê°™ë‹¤ ë¼ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì˜¤ë¥¸ìª½) ì´ë•Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Ft-1ì´ HR image informationì„ í¬í•¨í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ tain loss iteration ë§ˆë‹¤ loss connectí•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="../assets/img/SRFBN_01.png" alt="Alt text" /><br />
ì´ëŸ¬í•œ principle of feed-back scheme ëŠ” coarse SR image ê°€ better SR image ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡  LR image ë¥¼ refine í•´ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><strong><u>In summary, our main contributions are as follows:</u></strong><br /><br />
<strong><u>[1]</u></strong> : Feedback mechanism ì„ ì ìš©í•œ SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. networkëŠ” Feed-back connectionì„ í†µí•˜ì—¬ top-down feedback flowì—ì„œ high-level informationì„ ì œê³µí•©ë‹ˆë‹¤. parameterê°€ ê±°ì˜ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, strong early reconstruction abilityë¥¼ ì œê³µí•œë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p><strong><u>[2]</u></strong> : feed-back block ì„ ì œì•ˆí•©ë‹ˆë‹¤.(FB), í•´ë‹¹ blockì€ feedback information flowë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ 
handling í•  ìˆ˜ ìˆìœ¼ë©°, Up/Down sampling layer, dense skip connction ì„ í†µí•˜ì—¬, enriche high-level representationì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br /></p>

<p><strong><u>[3]</u></strong> : SR task ì—ì„œ curriculum training strategyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. SRì—ì„œ ì–´ë–»ê²Œ ë‚œì´ë„ë¥¼ ì¡°ì ˆí•˜ì˜€ëŠ”ì§€ê°€ ê¶ê¸ˆí•´ì§€ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.</p>

<h2 id="2-relatedwork">2. Relatedwork</h2>

<h3 id="-low-level-layer--high-level-layer-">[ low-level layer / high-level layer ]</h3>
<p><img src="../assets/img/SRFBN/SRFBN_05.png" alt="Alt text" />
ìœ„ì˜ VDSR êµ¬ì¡°ë¥¼ ë³´ë©´, ë‹¹ì—°íˆ low layerì˜ receptive field ëŠ” ì‘ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” SRFBNì˜ êµ¬ì¡°ëŠ” low-level layerê°€ high receptive fieldë¥¼ ë³´ëŠ” layer (high-level layer) ì˜ informationì— ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì´ low-level featureë¥¼ refine í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-feedback-mechanism-">[ Feedback mechanism ]</h3>
<p>Feed-back mechanismì€ previous layerì˜ ì •ë³´ë¥¼ refine í•˜ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” Feed-back mechanism ì„ ì—°êµ¬í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. [5,4,40,11,10,28] [11]ì€ ë³¸ ë¸”ë¡œê·¸ì— í¬ìŠ¤íŠ¸ ë˜ì–´ìˆëŠ” DBPN ì…ë‹ˆë‹¤.</p>

<p>ì €ìì˜ ë§ì— ë”°ë¥´ë©´,</p>
<blockquote> [11] designed up- and down-projection units to achieve iterative error feedback. Han et al. [10] applied a delayed feedback mechanism which transmits the information between two recurrent states in a dual-state RNN. However, the flow of information from the LR image to the final image is still feedforward in their network architectures unlike ours. </blockquote>
<p>DBPNì„ ë³´ë©´ Feedback mechanism ì„ ì‚¬ìš©í•œë‹¤ê³  í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ? Up/Down sampling ì„ í†µí•œ projectionìœ¼ë¡œ ë³´ê³  feed-foward ëœë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì ì—ì„œ LR to HR ì—ì„œ ì—¬ì „íˆ information flowì€ feed-fowardë˜ëŠ” ì ì„ ë‹¨ì ìœ¼ë¡œ ë³´ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p>ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ê³¼ ê´€ë ¨ì´ ê°€ì¥ ê¹Šì€ ì—°êµ¬ëŠ” [40]ì´ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ [40]ì—ì„œ ì„¤ëª…í•˜ëŠ” Feed-back based learning model ì…ë‹ˆë‹¤.
<img src="../assets/img/SRFBN/SRFBN_06.png" width="45%" height="45%" />
<img src="../assets/img/SRFBN/SRFBN_07.png" width="50%" height="50%" /><br /></p>

<h3 id="-b-single-upsampling-egfsrcnn-espcn-edsr-">[ (b) Single upsampling (eg.FSRCNN, ESPCN, EDSR) ]</h3>
<p>ì´ëŸ¬í•œ ë°©ì‹ì€ spatial resolution ì„ ë†’ì´ê³  (a)ì—ì„œ ì•ë‹¨ì—ì„œ ì‚¬ìš©í•œ interpolationê³¼ ê°™ì€ predefined operators ë¥¼ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ë²•ì—ì„œ Network capacity ê°€ ì œí•œë˜ì–´ ìˆì–´ ë³µì¡í•œ mappingì€ í•™ìŠµì— ì‹¤íŒ¨í•œë‹¤ê³  í•©ë‹ˆë‹¤. EDSR network ê°€ NTIRE2017ì—ì„œ ìš°ìŠ¹í•˜ì˜€ì§€ë§Œ, ë§¤ìš° ë§ì€ íŒŒë¼ë¯¸í„°ê°€ ìš”êµ¬ë©ë‹ˆë‹¤.</p>

<h3 id="-c-progressive-upsampling-eglapsrn-">[ (c) Progressive upsampling (eg.LapSRN) ]</h3>
<p>ì´ëŸ¬í•œ ë°©ì‹ì€ LapSRN ì—ì„œ ì²˜ìŒ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. networkì—ì„œ ì„œë¡œ ë‹¤ë¥¸ scaleë¡œ upsamplingí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ë‹¨ìˆœí•˜ê²Œ í•´ë‹¹ ë„¤íŠ¸ì›Œí¬ëŠ” limited LR featuresì— ì˜ì¡´í•˜ëŠ” single upsampling stackì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ LapSRNì€ lager scale factor x8 ì—ì„œ shallow networkë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.</p>

<h3 id="-d-iterative-up-and-downsampling-proposed-">[ (d) Iterative up and downsampling (proposed) ]</h3>
<p>ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” SR network ì…ë‹ˆë‹¤. ì €ìëŠ” different depth(different layer)ì™€ distribute ì—ì„œ SR featureì˜ sampling rateë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì¶˜ë‹¤ê³  í•©ë‹ë‹¤. (ê° stage ë§ˆë‹¤  reconstruction errorë¥¼ ê³„ì‚°)
í•´ë‹¹ schemaëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ë³´ë‹¤ deep featureë¥¼ ìƒì„±í•  ìˆ˜ìˆê²Œ í•˜ë©´ì„œ upsampling ì„ í•™ìŠµí•˜ì—¬ HR componentë¥¼ ë³´ì¡´í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-back-projection-">[ Back-projection ]</h3>
<p>[18] ì˜ Back-projectionì€ reconstruction error ë¥¼ minimize í•˜ëŠ” efficient iterative procedureë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ì—°êµ¬ë“¤ì—ì„œ back projectionì´ ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•˜ì˜€ìŠ¨ë¹„ë‹¤.
originally back-porjectionì€ Multi LR input ì´ ìˆëŠ” ê²½ìš° ì í•©í•˜ê²Œ designe ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. single LR inputì´ë¼ë©´, updating procedureì€ multiple upsampling operatorë¥¼ ì‚¬ìš©í•˜ê³  reconstruction error ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ê°€ëŠ¥í•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ì™€ ë¹„ìŠ·í•˜ê²Œ SR Task ì— ì ìš©í•œ ì—°êµ¬ë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤.
ì´ì „ ì—°êµ¬ë“¤ì„ í™•ì¥í•˜ì—¬ í•´ë‹¹ ë…¼ë¬¸ì€ SRì—ì„œ architectureë¥¼ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤.</p>

<h2 id="3-deep-back-projection-networks">3. Deep Back-Projection Networks</h2>

<p>ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” Projection unitì˜ êµ¬ì„±ì„ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.<br /></p>

<h3 id="-up-projection-uint-">[ Up-Projection Uint ]</h3>
<center><img src="../assets/img/DBPN/DBPN_05.png" width="50%" height="50%" /></center>
<p><br /></p>

<p>sacle up : (previously computed LR feature map) L^t-1 * spatial convolution operator <br />
scale down : scale up * spatial convolution operator<br />
residual : scale down - (previously computed LR feature map) L^t-1<br />
scale residual up : H1^t = residual * spatial convolution operator 
output feature map = Hx + Hx+1 â€¦â€¦<br /></p>

<p>ì´ë¥¼ ë„ì‹í™” í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.</p>
<center><img src="../assets/img/DBPN/DBPN_06.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>Up - Projection Unit ì€ L^t-1 = x[low-resolution] , H0^t = y[High-resoltion] ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ìµœì¢… outputì€ HR ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.</p>

<h3 id="-down-projection-uint-">[ Down-Projection Uint ]</h3>
<center><img src="../assets/img/DBPN/DBPN_08.png" width="50%" height="50%" /></center>
<p><br /></p>

<p>ì´ë¥¼ ë„ì‹í™” í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.</p>
<center><img src="../assets/img/DBPN/DBPN_07.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>UP-Projection Uint ê³¼ ìœ ì‚¬í•˜ê²Œ ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ìˆœì„œê°€ ë°”ë€ ê²ƒ ë¿ì…ë‹ˆë‹¤.</p>

<p>ê²°ê³¼ì ìœ¼ë¡œ, Projection Unitì€ projection errorë¥¼ sampling layerì— ì „ë‹¬í•©ë‹ˆë‹¤. projection errorë¥¼ feed-back í•©ë‹ˆë‹¤. ë°˜ë³µì ìœ¼ë¡œ self-correcting í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>Projection Uint ì€ large size filter(8,12)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ networkë“¤ì€ large size filterë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™œëƒë©´, networkì˜ convergence speed ê°€ ê°ì†Œí•˜ë©°, sub-optimal resultë¥¼ ìƒì„±í•  ìˆ˜ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ë°˜ë³µì ìœ¼ë¡œ Projection Uint ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ , large scale factor(x8)ì— ëŒ€í•´ shallow networkë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-dense-projection-uint-">[ Dense-Projection Uint ]</h3>
<p>Densenetì„ ì´ìš©í•˜ì—¬ Dense Projection Uint ë˜í•œ ì œì•ˆí•©ë‹ˆë‹¤.</p>
<center><img src="../assets/img/DBPN/DBPN_09.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>D-DBPN ê³¼ DBPNì˜ ì°¨ì´ì ì€ DBPNì€ ìµœì¢… outputì„ ë„ì¶œí•˜ê¸° ìœ„í•˜ì—¬ concatì„ í•˜ëŠ”ë°, Dense Projection Uint ì€ ìµœì¢… output ë¿ë§Œ ì•„ë‹ˆë¼, ì¤‘ê°„ì¤‘ê°„ MHR image ì—ë„ ëª¨ë‘ concat ëœ HR/LR image ê°€ inputìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤. ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ê²ƒì„ 1x1 conv ë¡œ ì–µì œí•˜ì˜€ìŠµë‹ˆë‹¤.</p>

<h3 id="-network-architecure-">[ Network architecure ]</h3>

<center><img src="../assets/img/DBPN/DBPN_10.png" width="100%" height="100%" /></center>
<p><br /></p>

<p>ì¤‘ê°„ì— back-projection unitì˜ ê°¯ìˆ˜ëŠ” ì¡°ì ˆì´ ê°€ëŠ¥í•¨ìœ¼ë¡œ ì €ìëŠ” ë…¼ë¬¸ì˜ network architectureê°€ module í˜•ì´ë¼ê³  í•©ë‹ˆë‹¤.
t stageê°€ ìˆëŠ” DBPN : inital extraction stage( 2 layer ) -&gt; t up-projection unit t-1 down-projection unit (each 3 layer) -&gt; reconstruction layer(one more layer)<br />
D-DBPN : conv(1x1)ì´ ì¶”ê°€ë©ë‹ˆë‹¤.</p>

<h2 id="4-experiment">4. Experiment</h2>

<p>ë…¼ë¬¸ì—ëŠ” ë” ë§ì€ ì‹¤í—˜ê²°ê³¼ ë° ì˜µì…˜ì´ ì •ì˜ ë˜ì–´ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ë©´ ë…¼ë¬¸ì„ ë³´ì…”ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<p>ìš°ì„ , í¥ë¯¸ë¡œìš´ê±´ H^të¥¼ ê°ê° visualize í•œ ì‹¤í—˜ê²°ê³¼ ì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ê·¸ë¦¼ì´ í•´ë‹¹ë©ë‹ˆë‹¤.</p>

<center><img src="../assets/img/DBPN/DBPN_11.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>ì €ìëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<blockquote>In Fig. 10, it is shown that each stage successfully generates diverse features to reconstruct SR image.</blockquote>

<p>ì•„ë˜ëŠ” ì² ì €íˆ ê°œì¸ì ì¸ ìƒê°ì´ë©° ì˜ê²¬ì…ë‹ˆë‹¤. <br />
í•´ë‹¹ ì‹¤í—˜ê²°ê³¼ê°€ ì¡°ê¸ˆ ë” ì‹ ë¹™ì„±ì´ ìˆìœ¼ë ¤ë©´, single upsampling ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.<br />
single upsampling network ì—ì„œ Depthì— ë”°ë¥¸ feature ì •ë³´ì™€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ê¶ê¸ˆí•˜ë‹¤ëŠ” ì˜ë¬¸ì„ ë‚¨ê²¨ë†“ìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ë©´, Deep network single upsampling ì—ì„œ ë³´ë‹¤ shallow network ì—ì„œ back-projectionì´ ë‚³ëŠ” ì´ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¶©ë¶„íˆ ë” ë³´ì—¬ì¤„ ìˆ˜ ìˆì§€ ì•Šì•˜ë‚˜ ë¼ëŠ” ìƒê°ì„ í•´ë´…ë‹ˆë‹¤.</p>

<center><img src="../assets/img/DBPN/DBPN_12.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>SR paper ê°€ ëª¨ë‘ ê·¸ëŸ°ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, cherry picking í•˜ì—¬ imageë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒì´ë¼ê³  ê°œì¸ì ìœ¼ë¡œ ìƒê°í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ x8 ì˜ ê²½ìš° ë‹¤ë¥¸ ê¸°ì¡´ì˜ networkì™€ í™•ì—°í•œ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ê¸° ë•Œë¬¸ì— ì €ìê°€ large scale factorì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤ëŠ”ê²ƒì— ì‹±ë¹™ì„±ì„ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.</p>

<center><img src="../assets/img/DBPN/DBPN_13.png" width="70%" height="70%" /></center>
<p><br /></p>

<p>[DBPN] : https://github.com/thstkdgus35/EDSR-PyTorch â€œIncludes implementation of DBPNâ€</p>

:ET