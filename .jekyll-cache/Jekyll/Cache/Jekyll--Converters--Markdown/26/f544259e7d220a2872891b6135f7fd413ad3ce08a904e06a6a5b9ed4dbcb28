I"o<p>안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</p>

<p>https://https://arxiv.org/abs/1504.08083 (ICLR 2018)</p>

<hr />

<h4 id="introduciton"><strong><u>Introduciton</u></strong></h4>

<p>당시 deep ConvNet[14,16]이 발전하면서 image classification과 object detection의 Acc가 상당히 향상 되었다고 합니다. <br />
당연히 classification보다 object detection이 복잡합니다. 때문에, [9,11,19,25]는 느리고 비효율적인 multi-stage pipline model을 train합니다.<br />
[14,16]은 Alex_net과 Backpropagation입니다.<br />
[9,11,19,25]는 R-CNN,SPP,Overfeat,segDeepM입니다.</p>

<p><strong><u>Multi-stage_pipline</u></strong><br />
R-CNN에서 3개의 module을 따로 학습 해야하는 것을 생각하면 될 것 같습니다.<br /></p>

<p><strong>논문에서는</strong> detection task가 object의 정밀한 localization을 필요로 하기 때문에 <u>두가지 issue</u>가 생긴다고 합니다.<br /></p>

<p><strong>[Issue1]</strong> : 수많은 객체 위치 후보들을 처리해야한다.<br />
이는 Speed,accuracy,simplicification을 손상시킵니다.<br />
<strong>[Issue2]</strong> : 정확한 localization을 위해 rough_localization을 다듬어야한다.<br /></p>

<p><strong>때문에 논문에서는</strong> [9,11]Network를 base로 하여 train과정을 단순화 합니다. (Sigle-stage traing algorthm을 제안합니다.) 결과적으로, R-CNN,SPP_net보다 VGGnet을 각각 9배,3배 빠르게 학습합니다.</p>

<p><strong><u>R-CNN의 단점</u></strong><br /></p>
<ol>
  <li>Multi-stage pipline(train)(Region proposal,SVM,bounding-box)</li>
  <li>SVM과 bounding-box regressor의 경우(train) 각 image의 proposal의 feature들이 disk에 기록됩니다. (VGG16의 경우 2.5일이 소요됩니다._VOC07)</li>
  <li>object detection이 느리다.</li>
</ol>

<p><strong>논문에서는</strong> Fast-R-CNN을 설명하기전, R-CNN과 SPPnet을 비교합니다.</p>
<ol>
  <li>SPPnet은 sharing compute를 이용하여 R-CNN의 속도를 높이기 위해 제안되었다.
    <ul>
      <li>여기서 말하는 sharing compute는 R-CNN과 달리 SPPnet은 한번의 convnet을 통과한 feature를 이용해 detection을 함으로써, end-to-end 학습이 된다는 이야기를 하는 것 입니다.</li>
    </ul>
  </li>
  <li>결과적으로, test:10~100배, train:3배 빠릅니다. <br /></li>
</ol>

<p><strong>그러나</strong>, SPPnet도 주목할만한 <strong>단점</strong>이 있습니다. (drawback)</p>
<ol>
  <li>SPPnet도 R-CNN과 마찬가지로, train은 multi-stage pipline(feature extraction/fine-tune(log_loss)/SVM train, bounding-box regressors 를 가집니다.) 또, feature가 disk에 기록됩니다.</li>
  <li>SPPnet에서 제안된 algorithm은 convolutional layer를 update할수 없고 convolution layer 전에 SPP를 할 수 없다.(network perpomance를 제한한다.)<br /></li>
</ol>

<p><strong><u>Contributions</u></strong><br /></p>

<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure4.png" width="1000" hight="300" /></center>
<center>(Fast-R-CNN)</center>
<p><br /></p>

<ol>
  <li>image의 region_proposal을 crop&amp;warp한 R-CNN과 달리 Fast-R-CNN은 입력이미지와 RoI_projection(object proposal)을 입력으로 받는다.</li>
  <li>network를 통해 convfeature map을 생성하기위해 전체 이미지를 처리한다.</li>
  <li>각 object proposal에 대해 RoIPooling layer가 고정길이 vector를 추출한다. (각 feature vector는 fc_layer에 연속적으로 전달된다.)</li>
</ol>

<p><strong><u>Multi-task loss</u></strong><br /></p>
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure10.png" width="1000" hight="300" /></center>
<center>(Multi-task loss)</center>
<p><br /></p>

<p><strong><u>RoI pooling layer</u></strong><br />
입력이미지에 한번만 CNN을 적용하고 RoI pooling을 이용하여 object 판별을 위한 feature를 추출하자는 것이 핵심이다.<br /></p>
<ul>
  <li>RoI로 추출할 feature size = HxW(eg.7x7)</li>
  <li>Feature map 위에 RoI의 좌표 (r,c,h,w)</li>
</ul>
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure7.gif" width="678" hight="300" /></center>
<center>(RoI pooling layer)</center>
<p><br />
Feature map위에서 h/H x w/W 만큼 Grid를 만들어 pooling 하면 결과적으로 원하는 HxW (첫 fc-layer와 호환이 되도록 하는 사이즈)feature size가 됩니다.<br />
(서로 다른 size의 region proposal이 입력되더라도 같은 size의 stride로 7x7을 만들어준다.)</p>

<p><strong><u>Initializing from pre-trained networks</u></strong><br /></p>

<p>논문에서는 pre-train된 ImageNet networks를 각각 실험합니다. pre-train network가 Fast-R-CNN을 initialize할때 3가지의 transform을 거칩니다.<br /></p>
<ol>
  <li>마지막 maxpooling layer를 RoI pooling layer로 대체 한다.</li>
  <li>network_last fully connected layer와 softmax layer는  two sibling layer로 대체됩니다.</li>
  <li>network는 two input 을 받도록 합니다. (image list와 해당 image의 RoI 목록)</li>
</ol>

<p><strong><u>Mini-batch sampling</u></strong><br /></p>

<p>fine-tuning에서, N=2(image),R=128 로 하여 R/N으로 image당 64RoI를 뽑습니다. CNN연산량을 줄이는 효과가 있습니다.       다음 그림과 같은 방식을 hierarchical sampling이라고 합니다.(계층적 샘플링)
Ground-truth와 IoU &gt; 0.5 (Positive), [0.1,0.5] 일 경우 Negative로 구분합니다. (즉, IoU가 너무 낮은것은 sample로 추가하지 않는다.) 
//train 할때, image horizontally flipped(p=0.5)의 augmentation만 사용하고 다른 augmentation 은 사용하지 않았다고 합니다. //</p>
<center><img src="/assets/images/posts/2019-10-04-Fast-R-CNN/figure6.png" width="678" hight="300" /></center>
<center>(Mini-batch sampling)</center>
<p><br /></p>

<h4 id="scale-invariance"><u>Scale invariance</u><br /></h4>
<p>논문에서 Scal invariance 를 해결하기 위해 2가지 방법을 사용하였습니다.<br /></p>

<p><strong>[brute force learning]</strong><br />
train/test 에서 미리 정해진 픽셀 크기로 처리된다. 때문에, network는 train_data 로 부터 정해진 scale의 detection을 해야합니다.<br /></p>

<p><strong>[using image pyramids]</strong><br />
image pyramid를 통해 network는 scale invariance를 제공한다. Test 에서는 image pyramid는 각region_proposal을 nomalize하는데 사용됩니다.<br /></p>

<p>논문에서는 GPU memory issue로 인하여 소규모 네트워크에 대해서만 multi_scale train을 하였다고 합니다. 즉, multi_scale에 관한 실험은 별로 진행하지 않았다는 것이며 논문의 내용은 scale invariance에 더 초점을 맞추고 있습니다.
<br /></p>

<h4 id="conclusion"><u>Conclusion</u></h4>
<p>저자가 말한대로 Fast-R-CNN은 R-CNN과 SPPnet과 다르게 깔끔함을 강조하고 있다.
Fast-R-CNN의 특징은 다음과 같다. <br /></p>

<ol>
  <li>이미지당 CNN을 한번만 수행한다.</li>
  <li>RoI pooling layer의 도입.(마지막 max-pooling 대신 사용되었다.)</li>
  <li>Classification 에서 SVM을 사용하던 것 과 달리 Softmax로 변경되었다. 즉, 하나의 network가 된것이다.</li>
  <li>Multitask loss 에서 보면 bounding Box regressor 또한 network의 부분이 되었다. <br /></li>
</ol>

<p><strong>이러한 특징을 바탕으로</strong> , R-CNN/SPPnet과 달리 mAP가 높고, multi-task loss 를 이용하여 single-satge train이 가능하고, end-to-end학습이 가능하고 feature caching을 하지 않으므로 disk 공간을 필요로 하지 않는다는 장점이 있다.</p>

<p>Fast-R-CNN의 다음 버전인, Faster-R-CNN은 region proposal 생성 방식의 개선을 주 논점으로 다룹니다.</p>

<h2 id="references">References</h2>
<ul>
  <li>R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.</li>
</ul>

:ET