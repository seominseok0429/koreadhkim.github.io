I"æA<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>https://arxiv.org/pdf/1903.09814 (CVPR 2019)</p>

<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>DBPNì—ì„œ ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì €ìëŠ” ê¸°ì¡´ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì€ HVS(human visual system)ì— ì¡´ì¬í•˜ëŠ” feedback mechanismì„ ì™„ì „íˆ í™œìš©í•˜ì§€ ëª»í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” low-level representationì„ high-level informationì„ ì´ìš©í•˜ì—¬ ê°œì„ í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë–„, feedback ë°©ì‹ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•˜ì—¬ hidden states (RNN)ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. feedback block ì€ feedback connectionì„ handeling í•  ìˆ˜ ìˆë„ë¡ designe ë˜ì—ˆê³ , ì´ëŠ” powerful high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ SRBFNì€ strong early reconstruction abilityë¥¼ ê°–ê³  ìˆìœ¼ë©°, step by step ìœ¼ë¡œ ìµœì¢… HR image ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” SR task ì—ì„œ curriculum learning ë°©ë²•ì„ í•™ìŠµì „ëµìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ì–´ë–»ê²Œ ì‚¬ìš©í•˜ì˜€ì§€ë„ êµ‰ì¥í•œ ê¶ê¸ˆì¦ì„ ìœ ë°œí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ì €ìëŠ” multiple type degradation taskì¸ SRì— ì í•©í•˜ë‹¤ê³  íŒë‹¨ í•˜ì˜€ìŠµë‹ˆë‹¤.
ë‹¤ìŒì˜ ê·¸ë¦¼ì€ RNN(Recurrent Neural Network)ì˜ hidden state ì…ë‹ˆë‹¤.<br />
<img src="/assets/img/SRFBN/SRFBN_02.png" alt="RNN hidden state" />
RNN ì˜ ê¸°ë³¸êµ¬ì¡°ì…ë‹ˆë‹¤. Green box = hidden state, Red box = input, Blue box = output ì…ë‹ˆë‹¤.
ì§ê´€ì ìœ¼ë¡œ ë³´ë©´, Green boxëŠ” inputê³¼ ë”ë¶ˆì–´ ì´ì „ term ì˜ Green boxë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ New Blue box(output)ì„ ìƒì„±í•œê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. CNNì—ì„œ ìƒê°í•´ë³´ë©´, layer ë¥¼ F(first) M(middel) F(final) ì´ë¼ê³  ìƒê°í•˜ë©´, Mt input = Ft + Mt-1 ì´ë¼ê³  ìƒê° í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>image super - resolution (SR) ì€ low-level computer vision task ì…ë‹ˆë‹¤. ëª¨ë“  ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ê²ƒ ì²˜ëŸ¼, SRì€ ì¼ë°˜ì ìœ¼ë¡œ 1 : N ì˜ ë‹µì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” Taskì…ë‹ˆë‹¤.(ill-posed problom), 1(LR) : N(HR) OR 1(HR) : N(LR), ë‘ ê²½ìš° ëª¨ë‘ í¬í•¨ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. 
ì´ì „ì— ì–´ë–¤ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì´ ìˆì—ˆëŠ”ì§€ ëŒ€ëµì ì¸ ë¶€ë¶„ì€ DBPN paper ì— ì˜ ì •ë¦¬ê°€ ë˜ì–´ìˆìŠµë‹ˆë‹¤.</p>

<p>ë‹¹ì—°íˆ network êµ¬ì¡°ê°€ ê¹Šì–´ì§ì— ë”°ë¼ parameterê°€ ì¦ê°€í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” overfitting ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.
parameterë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ recurrent structureë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. SR task ë„ ë§ˆì°¬ê°€ì§€ ì…ë‹ˆë‹¤. (eg.DRCN, DRRN) ì´ëŸ¬í•œ êµ¬ì¡°ì—ì„œ single-state Recurrent nuiral network(RNN)ìœ¼ë¡œ ì¶”ë¡ í•˜ì˜€ë‹¤ê³  í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ deep learning based methodì™€ ìœ ì‚¬í•˜ê²Œ recurrent structure networkë„ feed-forwoar ë˜ë©´ì„œ ì •ë³´ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ feed-forward ë°©ì‹ì€ skip connectionì„ ì‚¬ìš©í•˜ë”ë¼ë„ previous layerì—ì„œ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” layer ì˜ useful informationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë„ ë‹¹ì—°í•œ ê²ƒì´ ì¼ë²ˆì ìœ¼ë¡œ F(First) M(middel) F(final) ì˜ layer ê°€ ìˆìœ¼ë©´ ì¼ë°˜ì ì¸ feed-forward êµ¬ì¡°ì—ì„œëŠ” F(first)ëŠ” inputì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  M(middel)ì€ Fout(first output)ì´ ì…ë ¥ì´ë¯€ë¡œ F(first)ëŠ” Mout(middle output)ì„ ì ‘ê·¼ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>

<p>ì €ìëŠ” congnition theory ì—ì„œ feedback connectionì´ ê³ ì°¨ì› ì˜ì—­ìœ¼ë¡œ ì €ì°¨ì›ì˜ì—­ìœ¼ë¡œ ì‘ë‹µ ì‹ í˜¸ë¥¼ ì „ì†¡í•œë‹¤.ë¼ê³  í•˜ëŠ”ë°, ì‰½ê²Œë§í•´ì„œ, high-level feature ë¥¼ low-level layerì— ì „ë‹¬ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤. DBPNì—ì„œëŠ” Up/Down sampling Unit ì„ í†µí•˜ì—¬ feedback mechanismì„ ì ìš©í•˜ì˜€ëŠ”ë°, ì´ëŠ” ê°„ë‹¨í•˜ê²Œ Upsampling featureë¥¼ ë‹¤ì‹œ Down sampling í•˜ì—¬ low-level ë¡œ projectioní•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</p>

<p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” feed-back connectionì„ í†µí•˜ì—¬ high-level informationì„ low-level informationì„ refine í•˜ëŠ” SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ feed-back blockì„ ë³´ìœ í•˜ê³  ìˆëŠ” RNN êµ¬ì¡°ë¼ê³  í•©ë‹ˆë‹¤. í•´ë‹¹ network ëŠ” dense skip connectionì´ ìˆëŠ” ì—¬ëŸ¬ê°œì˜ Up/Down sampling ì„ í†µí•˜ì—¬ strong high-level representationì„ ìƒì„±í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>[40]ì—ì„œ ì˜ê°ì„ ë°›ì•„ì„œ FBì˜ outputì— hidden state in an unfolded RNNì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤ (achieve the feedback manner)</p>

<p><img src="../assets/img/SRFBN/SRFBN_03.png" alt="Alt text" />
<img src="../assets/img/SRFBN/SRFBN_04.png" alt="Alt text" /></p>

<p>ìœ„ì˜ ê·¸ë¦¼ì„ ì„¸ë¡œë¡œ ì„¸ìš°ë©´ ìœ„ì—ì„œ ì–¸ê¸‰í•œ RNN êµ¬ì¡°ì™€ ë˜‘ê°™ë‹¤ ë¼ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì˜¤ë¥¸ìª½) ì´ë•Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Ft-1ì´ HR image informationì„ í¬í•¨í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ tain loss iteration ë§ˆë‹¤ loss connectí•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><img src="../assets/img/SRFBN_01.png" alt="Alt text" /><br />
ì´ëŸ¬í•œ principle of feed-back scheme ëŠ” coarse SR image ê°€ better SR image ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡  LR image ë¥¼ refine í•´ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p><strong><u>In summary, our main contributions are as follows:</u></strong><br /><br />
<strong><u>[1]</u></strong> : Feedback mechanism ì„ ì ìš©í•œ SRFBNì„ ì œì•ˆí•©ë‹ˆë‹¤. networkëŠ” Feed-back connectionì„ í†µí•˜ì—¬ top-down feedback flowì—ì„œ high-level informationì„ ì œê³µí•©ë‹ˆë‹¤. parameterê°€ ê±°ì˜ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, strong early reconstruction abilityë¥¼ ì œê³µí•œë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p><strong><u>[2]</u></strong> : feed-back block ì„ ì œì•ˆí•©ë‹ˆë‹¤.(FB), í•´ë‹¹ blockì€ feedback information flowë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ 
handling í•  ìˆ˜ ìˆìœ¼ë©°, Up/Down sampling layer, dense skip connction ì„ í†µí•˜ì—¬, enriche high-level representationì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br /></p>

<p><strong><u>[3]</u></strong> : SR task ì—ì„œ curriculum training strategyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. SRì—ì„œ ì–´ë–»ê²Œ ë‚œì´ë„ë¥¼ ì¡°ì ˆí•˜ì˜€ëŠ”ì§€ê°€ ê¶ê¸ˆí•´ì§€ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.</p>

<h2 id="2-relatedwork">2. Relatedwork</h2>

<h3 id="-low-level-layer--high-level-layer-">[ low-level layer / high-level layer ]</h3>
<p><img src="../assets/img/SRFBN/SRFBN_05.png" alt="Alt text" />
ìœ„ì˜ VDSR êµ¬ì¡°ë¥¼ ë³´ë©´, ë‹¹ì—°íˆ low layerì˜ receptive field ëŠ” ì‘ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” SRFBNì˜ êµ¬ì¡°ëŠ” low-level layerê°€ high receptive fieldë¥¼ ë³´ëŠ” layer (high-level layer) ì˜ informationì— ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì´ low-level featureë¥¼ refine í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-feedback-mechanism-">[ Feedback mechanism ]</h3>
<p>Feed-back mechanismì€ previous layerì˜ ì •ë³´ë¥¼ refine í•˜ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” Feed-back mechanism ì„ ì—°êµ¬í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. [5,4,40,11,10,28] [11]ì€ ë³¸ ë¸”ë¡œê·¸ì— í¬ìŠ¤íŠ¸ ë˜ì–´ìˆëŠ” DBPN ì…ë‹ˆë‹¤.</p>

<p>ì €ìì˜ ë§ì— ë”°ë¥´ë©´,</p>
<blockquote> [11] designed up- and down-projection units to achieve iterative error feedback. Han et al. [10] applied a delayed feedback mechanism which transmits the information between two recurrent states in a dual-state RNN. However, the flow of information from the LR image to the final image is still feedforward in their network architectures unlike ours. </blockquote>
<p>DBPNì„ ë³´ë©´ Feedback mechanism ì„ ì‚¬ìš©í•œë‹¤ê³  í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ? Up/Down sampling ì„ í†µí•œ projectionìœ¼ë¡œ ë³´ê³  feed-foward ëœë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì ì—ì„œ LR to HR ì—ì„œ ì—¬ì „íˆ information flowì€ feed-fowardë˜ëŠ” ì ì„ ë‹¨ì ìœ¼ë¡œ ë³´ê³  ìˆìŠµë‹ˆë‹¤.</p>

<p>ë”°ë¼ì„œ, ë³¸ ë…¼ë¬¸ê³¼ ê´€ë ¨ì´ ê°€ì¥ ê¹Šì€ ì—°êµ¬ëŠ” [40]ì´ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ [40]ì—ì„œ ì„¤ëª…í•˜ëŠ” Feed-back based learning model ì…ë‹ˆë‹¤.
<img src="../assets/img/SRFBN/SRFBN_06.png" width="45%" height="45%" />
<img src="../assets/img/SRFBN/SRFBN_07.png" width="50%" height="50%" /><br /></p>

<p>ì €ìëŠ” ìƒë‹¨ìš°ì¸¡ ê·¸ë¦¼ì— í‘œì‹œëœ ConvLSTMì„ ì‚¬ìš©í•˜ì§€ë§Œ, SRFBNì˜ module(Feedback block(FB)) ë¡œì¨ ì •êµí•˜ê²Œ ì„¤ê³„í•˜ê² ë‹¤ê³  í•©ë‹ˆë‹¤. ë’¤ì—ì„œ ì‚´í´ë³´ê² ì§€ë§Œ, ConvLSTMë³´ë‹¤ SRì— ì í•©í•˜ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í•œë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h3 id="-curriculum-learning-">[ Curriculum learning ]</h3>
<p><img src="../assets/img/SRFBN/SRFBN_08.png" width="80%" height="80%" /><br />
ìœ„ì˜ ê·¸ë¦¼ìœ¼ë¡œ ì§ê´€ì ìœ¼ë¡œ ë³´ìë©´, í•™ìŠµëŒ€ìƒì˜ ë‚œì´ë„ë¥¼ ì ì  ë†’ì—¬ê°„ë‹¤ëŠ” ê²ƒì´ Curriculum learningì˜ ê¸°ì´ˆì…ë‹ˆë‹¤. ê·¸ë¦¼ì€ shapre recognitionì¸ë°, ì‰¬ìš´ sample ë¡œëŠ” ì •í™•í•œ ì›, ì •ì‚¬ê°í˜•ì„ ì‚¬ìš©í•˜ê³ , hard í•œ ìƒ˜í”Œë¡œëŠ” ì§ì‚¬ê°í˜•,íƒ€ì›ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” non-convexì—ì„œ ì§ˆì¢‹ì€ local-minimaë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì†Œê°œ ë˜ê³  ìˆê³ , ì´ëŠ” ë¹ ë¥¸ converge speedì™€ ì•ˆì •ì„±ì˜ ì´ì ì„ ê°€ì ¸ì˜¨ë‹¤ê³  ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ, SRì€ í•™ìŠµì´ ë¶ˆì•ˆì •í•œ Task ì¤‘ì— í•˜ë‚˜ì´ë©°, Adam optimizerë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤. ì´ëŸ°ì ì—ì„œ curriculum learningì´ SR domainì— ì í•©í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°ë©ë‹ˆë‹¤.</p>

<p>ì´ì™€ ê°™ì€ í•™ìŠµì „ëµì„ SR Task ì— ì ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. Curriculum learning ì—ì„œëŠ” ì‰¬ìš´ sample ì„ ì •ì˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ 2ê°€ì§€ë¥¼ ì œì‹œí•œ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. (1) ë…¸ì´ì¦ˆì˜ ê°œìˆ˜ë¡œ íŒë‹¨. (2) ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ë°”ìš´ë”ë¦¬ì—ì„œ margin ê±°ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. margin ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ ìˆ˜ë¡ ì‰½ê³ , margin ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ì–´ë µë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>SR task ì—ì„œëŠ” LR -&gt; HR ë¡œ non-linear mappingí•˜ëŠ” ê²ƒì¸ë°, HRì— noiseê°€ ìˆë‹¤ë˜ì§€ ë­”ê°€ LRí•˜ê³  ì¡°ê¸ˆì´ë¼ë„ ë¹„ìŠ·í•´ì§€ë©´ ë‹¹ì—°íˆ easy sample ì´ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œë„ easy sample ë¡œ HR(noise)ì„ ì‚¬ìš©í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<h2 id="3-feedback-network-for-image-sr">3. FeedBack Network for Image SR</h2>
<p>Two requirements are contained in a feedback system : <br />
<strong><u>(1) iterativeness</u></strong> <br />
<strong><u>(2) rerouting the output of the system to correct the input in each loop</u></strong>
<br />
ì´ëŸ¬í•œ, iterative cause-and-effect process ëŠ” ë…¼ë¬¸ì˜ principle of feedback scheme ì„ achieve í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤. ë…¼ë¬¸ì˜ feed-back scheme ë¥¼ achieve í•˜ëŠ”ë° í•„ìš”í•œ <u>ì„¸ ê°€ì§€ ë¶€ë¶„</u>ì´ ìˆëŠ”ë°, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.<br /></p>

<p><strong><u> (1) tying the loss at each iteration (to force the network to reconstruct an SR image at each iteration and thus allow the hidden state to carry a notion of high-level information) </u></strong> <br />
<strong><u> (2) using recurrent structure (to achieve iterative process)</u></strong><br />
<strong><u> (3) providing an LR input at each iteration (to ensure the availability of low-level information, which is needed to be refined) </u></strong> <br /></p>

<p>ì—¬ê¸°ì„œ ë³´ë©´, *loss tying, *recurrent structure, *providing LR input ì´ ëˆˆì— ë„ëŠ”ë°, ì´ëŠ” ì‰½ê²Œë§í•´ì„œ, ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë„¤íŠ¸ì›Œí¬ê°€ SR taskì—ì„œ ì˜ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ ë¶€ë¶„ì„ ë‚˜ì—´í•œ ê²ƒì…ë‹ˆë‹¤.<br />
(1)loss typing ì€ Hidden stateë¥¼ Hë¼ê³  í•˜ë©´, Ht-1ì´ ì˜¨ì „íˆ Ht-1 informationì„ low-level featureë¡œ ì „ë‹¬í•˜ë ¤ë©´ ê·¸ ë¶€ë¶„ì—ì„œ lossë¥¼ ë¬¶ì–´ weightê°€ ê°±ì‹ ë˜ì§€ ì•Šê²Œ í•´ì•¼ í•œë‹¤. ë¡œ ì´í•´ ë  ìˆ˜ ìˆì„ ê²ƒê°™ìŠµë‹ˆë‹¤.( ì €ì˜ ê´€ì ì—ì„œ ë³´ì´ëŠ” ê²ƒì€ ê·¸ë ‡ìŠµë‹ˆë‹¤.)<br />
(2)ì €ìëŠ” ë…¼ë¬¸ì—ì„œ RNNì˜ ì§ê´€ì— ë™ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‹¹ì—°í•´ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” feed-back mechansimì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤.<br />
(3) iteration ë§ˆë‹¤ LR inputì„ providing í•˜ëŠ”ë°, ì´ëŠ” LRì— ëŒ€í•œ ì •ë³´ë¥¼ ìƒì§€ ì•Šìœ¼ë©´ì„œ, refineëœ ê²ƒë“¤ì„ ì „ë‹¬í•˜ê¸° ìœ„í•¨ìœ¼ë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤.<br /></p>

<h3 id="-network-structure-">[ Network structure ]</h3>

<p><img src="../assets/img/SRFBN/SRFBN_10.png" alt="Alt text" /><br /></p>

<p>ì•ì „ì— ë´¤ë˜ ê·¸ë¦¼ì…ë‹ˆë‹¤. êµ¬ì¡°ë¥¼ ë³´ì‹œë©´, t=1 ~ t=Tê¹Œì§€ ë°˜ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ ì „ í™•ì¸í•œ ê²ƒê³¼ ê°™ì´ loss tyingìœ¼ë¡œ ì˜¨ì „í•œ hidden stateë¥¼ ì „ë‹¬í•œë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ëƒ¥ ê°„ë‹¨í•˜ê²Œ, t=1ì—ì„œ outputì„ ê·¸ëŒ€ë¡œ t=2ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ì„œ loss tying í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì €ìëŠ” networkë¥¼ three partë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.<br />
<strong><u>(part 01) LRFB</u></strong> <br />
<img src="../assets/img/SRFBN/SRFBN_11.png" alt="Alt text" /><br /></p>

<p><strong><u>(part 02) FB</u></strong> <br />
<img src="../assets/img/SRFBN/SRFBN_12.png" alt="Alt text" /><br /></p>

<p><strong><u>(part 03) RB</u></strong> <br />
<img src="../assets/img/SRFBN/SRFBN_13.png" alt="Alt text" /><br />
<img src="../assets/img/SRFBN/SRFBN_14.png" alt="Alt text" /><br /></p>

<p>ìˆ˜ì‹ì´ ë„ˆë¬´ë‚˜ ê°„ë‹¨í•˜ê³  ê·¸ë¦¼ê³¼ ê°™ì´ ë³¸ë‹¤ë©´ ì§ê´€ì ìœ¼ë¡œ ë³´ì´ê¸° ë•Œë¬¸ì— ì–´ë ¤ì›€ì€ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. LR image residual ì—ì„œëŠ” bilinear upsample kernel ì„ ì‚¬ìš©í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ì „ì— ì €ìê°€ ì„¤ëª…í•œ ëŒ€ë¡œ LR inputì˜ informationì„ ë„˜ê²¨ì£¼ê¸° ìœ„í•´ residual ì„ ì‚¬ìš©í•œë‹¤ê³  í•˜ì˜€ìœ¼ë¯€ë¡œ Deconvë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì´ìœ ê°€ ì„¤ëª…ì´ ë˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h3 id="-feedback-blockfb-">[ Feedback block(FB) ]</h3>

<p><img src="../assets/img/SRFBN/SRFBN_15.png" alt="Alt text" /><br /></p>

<p>Feedback block(FB) ì—ëŠ” G projection group sequence ë¥¼ í¬í•¨í•˜ë©°, ì‚¬ì´ì‚¬ì´ì— dense skip connection ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. G projection group ì—ëŠ” Up/Down sampling ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.<br /></p>

<p>ìœ„ì˜ FBì—ì„œ ì´ì „ ì¶œë ¥ê³¼ í˜„ì¬ ì…ë ¥ì€ concat ë©ë‹ˆë‹¤. concat í›„ Conv(1,m)ì„ ì´ìš©í•˜ì—¬ ì´ì „ ì¶œë ¥ê³¼ í˜„ì¬ ì…ë ¥ì„ refine í•©ë‹ˆë‹¤.
ì´ëŠ” ìˆ˜ì‹ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ê³  ë‚˜ë¨¸ì§€ ìˆ˜ì‹ë“¤ë„ ê·¸ë¦¼ê³¼ í•¨ê»˜ ë³´ë©´ ì´í•´í•˜ê¸° í¸í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br />
<img src="../assets/img/SRFBN/SRFBN_16.png" alt="Alt text" /><br />
<img src="../assets/img/SRFBN/SRFBN_17.png" alt="Alt text" /><br />
<img src="../assets/img/SRFBN/SRFBN_18.png" alt="Alt text" /><br />
<img src="../assets/img/SRFBN/SRFBN_19.png" alt="Alt text" /><br /></p>

<p>ê²°ê³¼ì ìœ¼ë¡œ outputì€ ë°”ë¡œ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ë°, ë…¼ë¬¸ì—ì„œëŠ” feature fusionì´ë¼ê³  í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‰½ê²Œ ë§í•´ì„œ, HR space ëŠ” HR space ë¼ë¦¬, LR space ëŠ” LR space ë¼ë¦¬ ì—°ì‚°ì„ í•˜ê² ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<h3 id="-curriculum-learning-strategy-">[ Curriculum learning strategy ]</h3>

<p>loss ëŠ” ë‹¤ìŒê³¼ ê°™ì´ L1 loss ì— curriculum learningì„ ì‚¬ìš©í•œ ê²ƒìœ¼ë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤.
detail í•œ ì •ë³´ëŠ” ë…¼ë¬¸ì—ì„œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</p>

<p><img src="/assets/img/SRFBN/SRFBN_09.png" alt="Alt text" /><br /></p>

<h3 id="-implementation-details-">[ Implementation details ]</h3>

<p>ì‹¤ì œ implementated code ê°€ ìˆê¸´í•˜ì§€ë§Œ, ì§ì ‘ í•´ë³´ì‹¤ ë¶„ë“¤ì€ ì°¸ê³ í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br />
(1) activation fuction = PReLU <br />
(2) scale factor 2 : Conv(k,m)  Deconv(k,m) â€“&gt; k=6 (stride=2, padding=2)<br />
(3) scale factor 3 : Conv(k,m)  Deconv(k,m) â€“&gt; k=7 (stride=3, padding=2)<br />
(4) scale factor 4 : Conv(k,m)  Deconv(k,m) â€“&gt; k=8 (stride=4, padding=2)<br /></p>

<h2 id="4-experimental-result">4. Experimental Result</h2>

<h3 id="-experimental-preveiw-">[ Experimental preveiw ]</h3>

<p>training dataset = DIV2K, Flicker2K <br />
benchmark = Set5, Set14, B100, Urban100, Manga109<br />
ì´ì „ networkì™€ ë™ë“±ë¹„êµ í•˜ê¸° ìœ„í•˜ì—¬ Y(luminance) channel ì—ì„œ ë§Œ evaluation í•©ë‹ˆë‹¤. <br /></p>

<p>Degradation models : BD / DN ì—ì„œì˜ evaluationë„ ì§„í–‰í•©ë‹ˆë‹¤.<br />
BD : HR image â€“&gt; + Gaussian blur(7x7) â€“&gt; + downsampling<br />
DN : HR image â€“&gt; + downsampling â€“&gt; + Gaussian noise(30 level)<br /></p>

<p>input patch size ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. patch size ë¥¼ ì™œ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•˜ì˜€ëŠ”ì§€ëŠ” ì„¤ëª…ì´ ë˜ì–´ ìˆì§€ì•Šì€ë°, ì•„ë¬´ë˜ë„ patch size ë˜í•œ hyperparameter ë¡œì¨ ì°¾ì€ ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤.<br />
<img src="/assets/img/SRFBN/SRFBN_20.png" alt="Alt text" /><br /></p>

<p><img src="/assets/img/SRFBN/SRFBN_21.png" alt="Alt text" /><br /></p>

<p>ìœ„ì˜ ì‹¤í—˜ì€ T,G ì˜ ê´€ê³„ì„±ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ í•˜ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë°˜ë³µíšŸìˆ˜(T)ì™€ projection group (G)ëŠ” ë§ì„ ìˆ˜ë¡ ì¢‹ë‹¤. ê°€ ê²°ë¡ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, Tì˜ ìƒìŠ¹ì— ë”°ë¥¸ ì„±ëŠ¥ë³€í™”ê°€ Gì˜ ìƒìŠ¹ì— ë”°ë¥¸ ì„±ëŠ¥ë³€í™”ë³´ë‹¤ ëšœë ·í•œ ê²ƒìœ¼ë¡œ ë³´ì•„ ë°˜ë³µíšŸìˆ˜(T)ê°€ ì„±ëŠ¥ì— ë” ë§ì€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<p>[DBPN] : https://github.com/thstkdgus35/EDSR-PyTorch â€œIncludes implementation of DBPNâ€</p>

:ET