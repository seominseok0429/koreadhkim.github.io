I"<h3 id="-안녕하세요-인공지능-공부연구중인-김대한-이라고-합니다-이번-포스트는-다음의-논문과-연관이-있습니다"><strong> 안녕하세요. 인공지능 공부/연구중인 김대한 이라고 합니다. 이번 포스트는 다음의 논문과 연관이 있습니다.</strong></h3>
<p>https://arxiv.org/pdf/1909.11856.pdf (ACM MM 2019)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>논문에서 저자는 과도한 convolution의 사용은 low computing power device에서 사용에 제한이 있다는 말을 꺼내고 있습니다. 또한, arbitrary scale factor 는 previous approaches에서는 해결되지 않았던 practical application에 적용하기 위해 중요한 issue라고 합니다. 위와 같은 문제를 해결하기 위하여, lightweight information multi-distillation network(IMDN) 을 제안합니다. IMDN은 information multi distillation blocks(IMDB)로 구성되어 있으며, 이를 cascade 구조로 construction한다고 합니다. IMDB 에는 distillation/ selective fusion part가 있다고 합니다.
<br /></p>

<p><strong>distillation module : extracts hierarchical features step-by-step</strong><br />
<strong>fusion module : aggregates them according to the importance of candidate features</strong><br /></p>

<p>이 두가지 모듈은 proposed contrast-aware channel attention mechanism 으로 evalution된다고 합니다.<br /></p>

<p>한편, real image(any sizes)를 처리하기 위해서, block-wise-image patches를 super-resolve 하는 Adaptive cropping strategy(ACS)를 develop 하였다고 합니다. <br /></p>

<p>논문의 초록을 보면 이번 논문에서 무슨 문제를 어떻게 해결하겠는지가 어느정도 보입니다. 이번 논문에서는 low-computing power device에서도 사용가능한 shallow 하면서도 성능이 좋은 network 구조를 제안할 것으로 보이고, practical application에 적용하기 위한 어떤 방법을 제안할 것으로 보입니다.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>다수의 SR논문에서 (eg.SISR) Introduction에서 말하는 내용은 비슷합니다.(제가 읽은 SISR에 한해서)<br />
(1) LR –&gt; HR Task이다.<br />
(2) ill-posed problem이다.<br />
(3) 이러한 issue 를 해결하기 위해 많은 방법이 제안되었다.<br />
(4) (3)에서 말한 많은 방법들에 대한 맛보기.<br />
(5) (4)에서 말한 맛보기에서 issue들을 나열<br /></p>

<p>저자는 DRRN, MemNet을 예시로 들어, 이들을 분석한 결과를 제시하고 있습니다. 아래의 그림은 차례로 DRRN, MemNet입니다.<br />
<img src="../assets/img/IMDN/IMDN_01.png" alt="Alt text" /><br />
<img src="../assets/img/IMDN/IMDN_02.png" alt="Alt text" /><br /></p>

<p>저자는, DRRN, MemNet의 Train/Test time이 너무 오래걸리는 것과, network가 GPU memory를 많이 소모하는것에 집중합니다. 저자의 분석은 DRRN, MemNet이 느리고, GPU memory를 많이 사용하는 이유는 input자체가 LR(interpolated) 이며, network 내부에서 downsampling operation을 하지 않아서 라고 합니다. DBPN에 정리가 잘 되어있는데, LR(interpolated)를 선택하여 model을 설계하는것은 계산적인 부분에서 매우 단점으로 작용하게 됩니다. HR space에서 SR이 되기 때문입니다. 그래서 해당 문제를 해결하기 위하여, LR input을 그대로 network에 넣고, deconv, pixel-shuffle 등을 이용하여 network 끝 혹은 중간에, Upsampling 하는 방법들이 제시가 되었습니다. Introduction에서 아주 잘 설명된 부분이 있는데, 바로 아래와 같습니다.<br /></p>

<p><img src="../assets/img/IMDN/IMDN_03.png" alt="Alt text" /><br /></p>

:ET