I"«3<h3 id="-ì•ˆë…•í•˜ì„¸ìš”-ì¸ê³µì§€ëŠ¥-ê³µë¶€ì—°êµ¬ì¤‘ì¸-ê¹€ëŒ€í•œ-ì´ë¼ê³ -í•©ë‹ˆë‹¤-ì´ë²ˆ-í¬ìŠ¤íŠ¸ëŠ”-ë‹¤ìŒì˜-ë…¼ë¬¸ê³¼-ì—°ê´€ì´-ìˆìŠµë‹ˆë‹¤"><strong> ì•ˆë…•í•˜ì„¸ìš”. ì¸ê³µì§€ëŠ¥ ê³µë¶€/ì—°êµ¬ì¤‘ì¸ ê¹€ëŒ€í•œ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ë‹¤ìŒì˜ ë…¼ë¬¸ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.</strong></h3>
<p>http://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Embedded_Block_Residual_Network_A_Recursive_Restoration_Model_for_Single-Image_ICCV_2019_paper.pdf (ICCV 2019)</p>
<hr />

<h2 id="0-abstract">0. Abstract</h2>
<p>SISR(Single Image Super Resolution)ì€ LR image ë¡œ ë¶€í„° lost structures and textures ë¥¼ restoreí•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ë‹¹ domainì— ë§ì€ ê´€ì‹¬ì„ ëŒì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì•Œë‹¤ì‹œí”¼ SRì—ì„œ top performers ëŠ” deep / wide CNN ì„ í¬í•¨í•©ë‹ˆë‹¤. ë˜ëŠ” DBPN/SRFBNê³¼ ê°™ì€ recurrent êµ¬ì¡°ë¥¼ ê°€ì§€ê¸°ë„ í•©ë‹ˆë‹¤. ì €ìëŠ” ì´ëŸ¬í•œ method ë“¤ì´ textures and structuresë¥¼ ë³µì›í•˜ê¸° ìœ„í•´ì„œ single modelì„ ì‚¬ìš©í•˜ëŠ”ë° ë¬¸ì œê°€ ìˆë‹¤ê³  ë§í•©ë‹ˆë‹¤. ì™œ? ë¬¸ì œê°€ ìˆë‹¤ê³  íŒë‹¨í•˜ì˜€ì„ê¹Œìš”? ì €ìëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§í•©ë‹ˆë‹¤.</p>

<blockquote> A typical operation is that a certain layer restores the textures based on the ones recovered by the preceding layers, ignoring the characteristics of image textures. In this paper, we believe that the lowerfrequency and higher-frequency information in images have different levels of complexity and should be restored by models of different representational capacity
</blockquote>

<p>ì¦‰, ê¸°ì¡´ì˜ network ë“¤ì€ ignoring the characteristic of image textures. í•˜ê³ , ì´ì „ì˜ layer ì—ì„œ recover ëœ ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ layerì—ì„œ ë³µì›í•˜ëŠ” ê²ƒì´ ë¬¸ì œë¼ê³  í•©ë‹ˆë‹¤. ì‚¬ì‹¤ ì´ë¶€ë¶„ì—ì„œëŠ” ì˜êµ¬ì‹¬ì´ ìƒê¸¸ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë¬¼ë¡  í•´ì„í•˜ê¸° ë‚˜ë¦„ì´ì§€ë§Œ, DBPN/SRFBN ê°™ì€ recurrent structure ì˜ ê²½ìš°, ê¸°ë³¸ì ìœ¼ë¡œ baseê°€ ë˜ëŠ” ì§ê´€ì€ low-level featrueë¥¼ ì¢€ë” ì •êµí•˜ê²Œ ë‹¤ë“¬ì–´ ë³´ì. ë¼ê³  ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ë°, ì €ìì˜ ë§ì„ ì¸ìš©í•˜ë©´ ì´ì „ layerì—ì„œ recoverëœ ì •ë³´ë¥¼ ê°€ì§€ê³  restore í•˜ëŠ” ê²ƒë³´ë‹¤ LR imageì—ì„œ ì„œë¡œ ë‹¤ë¥´ê²Œ extractioní•˜ëŠ” ê²ƒì´ ì¢‹ì§€ ì•Šì„ê¹Œ? ë¼ëŠ” ë§ë¡œ í•´ì„í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë˜ëŠ”, ì´ì „ layerì—ì„œ recover ëœ ì •ë³´ë¡œ ë‹¤ìŒ Layerì—ì„œ ë°”ë¼ë³´ëŠ” ê²ƒì´ ì •ë§ image texturesë¥¼ ë¬´ì‹œí•œë‹¤. ë¼ê³  ë³¼ ìˆ˜ ìˆì„ê¹Œ? ë¼ëŠ” ìƒê°ì´ ë“­ë‹ˆë‹¤.</p>

<p>ì‰½ê²Œ ë§í•´ì„œ, ì €ìëŠ” low-frequency information / high-frequency informationì´ ì„œë¡œ different í•˜ë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì„œë¡œë‹¤ë¥¸ capacityë¥¼ ê°€ì§„ ëª¨ë¸ì—ì„œ restore í•´ì•¼í•œë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br /></p>

<p>ê·¸ë˜ì„œ ì €ìëŠ” incremental recovering progress for texture super-resolution ì¸ EBRN (embedded block residual network) ì„ ì œì•ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” moduelë“¤ì€ ê°ê° ë‹¤ë¥¸ frequency inforamtion ë¥¼ restore í•œë‹¤ê³  í•©ë‹ˆë‹¤. resotre í•˜ëŠ” ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.<br />
<strong>low-frequency inforamtion â€“&gt; shallow moduel</strong><br />
<strong>high-frequency information â€“&gt; deeper moduel</strong><br /></p>
<h2 id="1-introduction">1. Introduction</h2>
<p>SR ì€ ill-posed problem ì¸ ê²ƒì€ ëª¨ë‘ ë‹¤ ì•Œê³  ê³„ì‹¤ê²ë‹ˆë‹¤. ë…¼ë¬¸ì€ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<blockquote>To overcome this issue, most methods such as those based on deep convolutional neural networks constrain the solution space by learning a mapping function from external lowand high-resolution exemplar pairs or by involving a priori knowledge on the HR feature space.</blockquote>

<p>ì—¬ê¸°ì„œ external / internal method ê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ì•¼í•  í•„ìš”ì„±ì´ ìˆì„ ê²ƒ ê°™ì€ë°, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.<br />
external method :<br />
internal method :<br /></p>

<p>Learning-based methods ì—ì„œ deep or wide convolutional neural network ëŠ” high-representational capacity ë•Œë¬¸ì— top performerë¡œì¨ ìë¦¬ë§¤ê¹€ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜, ì €ìëŠ” desiged network ì˜ ì„±ëŠ¥í–¥ìƒì€ parameterì˜ ì¦ê°€ì™€, connectionì˜ ì •êµí•¨(ì„¸ë°€í•¨)ì—ì„œ ë¹„ë¡¯ëœë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¹ì—°íˆ parameterì˜ ì¦ê°€ì™€, connectionì˜ ì •êµí•¨ì— ë”°ë¥¸ ê³„ì‚°ëŸ‰,ë©”ëª¨ë¦¬ì˜ ë¬¸ì œê°€ ìˆë‹¤ë©´ Real-world applicationì—ì„œëŠ” ì‚¬ìš©í•˜ê¸° í˜ë“­ë‹ˆë‹¤.</p>

<p>ì €ìëŠ” ì´ëŸ¬í•œ ë¬¸ì œì ì€ deep-model based methods ê°€ ê¸°ì¡´ì— ë„ë¦¬ ì•Œë ¤ì§„ image frequencyì˜ íŠ¹ì„±ì„ ì˜ ê³ ë ¤í•˜ì§€ ì•Šê³  ì„¤ê³„ë¬ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ ê·¸ë¦¼ìœ¼ë¡œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
<img src="../assets/img/EBRN/EBRN_01.png" alt="Alt text" /><br /></p>

<p>ì—¬ê¸°ì„œ, image frequency ê°€ ì™œ? ì¤‘ìš”í•˜ëƒ? ì— ëŒ€í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.<br />
netural imageëŠ” low-frequency / high-frequency information ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆê³ , ê°ê°ì€ ì„œë¡œ ë‹¤ë¥¸ structures and textures complexityë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤. SISRì„ í¬í•¨í•œ image restore Taskì—ì„œëŠ” low/high frequency informationì„ ë³µì›í•˜ê¸° ìœ„í•´ì„œ íŠ¹ì •í•œ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.
ê²°ê³¼ì ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. abstract ì—ì„œ ì–¸ê¸‰í•œ shallow moduel, deeper moduelì„ ì‚¬ìš©í•˜ê² ë‹¤.! ë¼ê³  í•œ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.<br /></p>
<h3 id="low-frequency-inforamtion--simple--simple-complex-restoring-function--shallow-moduel"><strong>low-frequency inforamtion â€“&gt; simple â€“&gt; simple complex restoring function â€“&gt; shallow moduel</strong></h3>
<h3 id="high-frequency-information--hard--more-complx-restoring-fucntion--deeper-moduel"><strong>high-frequency information â€“&gt; hard â€“&gt; more complx restoring fucntion â€“&gt; deeper moduel</strong></h3>

<p>VDSR êµ¬ì¡°ë¥¼ ìƒê°í•´ë³´ë©´ ìƒëŒ€ì ìœ¼ë¡œ low-level layer ëŠ” low-frequency informationì— fit í•˜ì§€ë§Œ, high-frequency informationì—ëŠ” underfit í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. high-level layerì˜ ê²½ìš°ëŠ” ê·¸ ë°˜ëŒ€ì— í•´ë‹¹ë  ê²ƒì…ë‹ˆë‹¤.
ì´ëŠ” ë‹¹ì—°í•œê²ƒì´ LR - HR image ì‚¬ì´ì— ì°¨ì´ê°€ ë¶„ëª…í•œ ë¶€ë¶„ì€ high-frequency informationì´ê³  networkëŠ” high-frequency informationì„ ì˜ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
<br /> ë˜í•œ, ì €ìëŠ” model architecture ì™€ frequency bands(low/high) ì‚¬ì´ì— connectionì„ ìœ„í•´ì„œëŠ” elaboration residual ideaê°€ í•„ìš”í•˜ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.(ê¸°ì¡´ì˜ residual ì€ elaborationí•˜ì§€ ì•Šë‹¤ ë¼ê³  ë³´ê³  ìˆìŠµë‹ˆë‹¤.)</p>

<p><img src="../assets/img/EBRN/EBRN_02.png" alt="Alt text" /><br />
ì €ìëŠ” figure 1 ì„ ë°”íƒ•ìœ¼ë¡œ (í™•ëŒ€ëœ ë¶€ë¶„) EDSRì—ì„œëŠ” ì±…ì˜ textureë¥¼ ì™„ë²½íˆ ë³µì›í•˜ì§€ ëª»í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì •ì„±ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©´ì„œ, ì´ëŠ” EDSR ì—ì„œ residual ì„ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ì§€ ëª»í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.  í•´ë‹¹ figure ë¥¼ ë°”íƒ•ìœ¼ë¡œ EDSRì€ high-level layerê°€ low-frequency informationì— over-fit í•˜ê³  ìˆë‹¤.ë¼ë©° EDSRì´ ë‹¨ì ì´ ìˆë‹¤ëŠ” ì ì„ ê¼¬ì§‘ê³  ìˆìŠµë‹ˆë‹¤.<br /></p>

<p>ì €ìëŠ” ìœ„ì˜ ê·¼ê±°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ different complexityë¥¼ ê°–ëŠ” ê°ê°ì˜ sub-networkë“¤ì„ ì´ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ frequency ì— ì¡´ì¬í•˜ëŠ” EBRNì„ ì œì•ˆí•©ë‹ˆë‹¤.</p>

<p>ì €ìëŠ” ì´ êµ¬ì¡°ëŠ” êµ¬ì²´ì ìœ¼ë¡œ, BRMì„ ê¸°ë³¸ moduleë¡œ ì‚¬ìš©í•˜ê³ , super resolution flow / back-projection flow ë‘ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤ê³  í•©ë‹ˆë‹¤. ê°ê°ì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.<br />
<strong>super resolution flow</strong> : low-frequency structures and textures recover <br />
<strong>back-projection flow</strong> : high-frequency structures and textures recover <br /></p>

<p>ì „ì²´ì ì¸ model ì€ ì—¬ëŸ¬ê°œì˜ BRMì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, Each BRM is stacked on the back-projection flow of its antecedent BRM. ì´ë¼ê³  í•©ë‹ˆë‹¤.</p>

<p>BRM ì€ low-frequency information recover ë¥¼ ë‹´ë‹¹í•˜ë©°, high-frequency BRM(high-level-layer)ì—ê²Œ ì´ë¥¼ pass í•œë‹¤ê³  í•©ë‹ˆë‹¤. ëª¨ë“  BRM outputì„ fuse í•˜ê¸° ìœ„í•´ì„œ, recurrent fusion technique ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” feature flow / gradient flowë¥¼ stabilize í•˜ê³ , convergenct speedë¥¼ ë†’ì´ê¸° ìœ„í•¨ì´ë¼ê³  í•©ë‹ˆë‹¤.</p>

<p><strong><u>In summary, the main contributions of this work are as follows:</u></strong><br />
<strong><u>[1]</u></strong> : ì„œë¡œë‹¤ë¥¸ frequency information(low/high)ëŠ” different complexityë¥¼ ê°–ëŠ” modelì—ì„œ restore ë˜ì•¼í•˜ëŠ” ê²ƒì´ë¼ëŠ” motivationì„ ì œì•ˆí•©ë‹ˆë‹¤. ì €ìëŠ” bad-caseì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì„ ìˆ˜ë„ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.<br />
<strong>deep model â€“&gt; low-frequency information over-recoverd</strong><br />
<strong>shallow model â€“&gt; high-frequency information under-recoverd</strong><br />
<strong><u>[2]</u></strong> : BRM(block residual module)ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” hard-to-recoverd informationì„ deep moduleë¡œ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ëŠ” image structures and texturesë¥¼  ì˜ ë³µì›í•˜ë„ë¡ í•œë‹¤ê³  í•©ë‹ˆë‹¤.
<strong><u>[3]</u></strong> : multiple BRM embeedding techniqueì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ê° module ì˜ outputì„ ë°”íƒ•ìœ¼ë¡œ reconstruction qualityë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.</p>

<h2 id="2-related-work">2. Related work</h2>
<p>ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì„ ë§í•˜ê¸°ì—ëŠ” page ìˆ˜ê°€ ì œí•œë˜ì–´ deep mothod ë§Œ ì„¤ëª…í•œë‹¤ê³  í•˜ëŠ”ë°, ë…¼ë¬¸ì—ì„œ pageìˆ˜ê°€ ì œí•œë˜ì–´ì„œ ì•ˆì ëŠ”ë‹¤ëŠ” ë§ì€ ì²˜ìŒ ë´ì„œ ì‹ ê¸°í–ˆìŠµë‹ˆë‹¤.</p>

<p>ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì— ëŒ€í•´ì„œ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•„ìš”í•˜ì‹œë©´ paper ì—ì„œ ì§ì ‘ ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. x8 scale factorì—ì„œ ìµœê·¼ ì£¼ëœ ê´€ì‹¬ì„ ë°›ê³  ìˆìŒì„ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•Œê³  ê°€ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ x2,x3,x4ì˜ ê²½ìš° ìƒˆë¡œìš´ ë…¼ë¬¸ì—ì„œë„ ì„±ëŠ¥í–¥ìƒí­ì´ ë‚®ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ëŠ” saturateë¬ë‹¤.ë¼ê³  ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.</p>

<p>ê²°ê³¼ì ìœ¼ë¡œ ì €ìëŠ” introductionì—ì„œ ë§í–ˆë“¯ì´, ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ë“¤ì´ ë†“ì¹˜ê³  ìˆëŠ” low/high frequency ë¥¼ ë‹¤ë¥´ê²Œ ë‹¤ë¤„ì„œ ì„±ëŠ¥í–¥ìƒì„ ì´ëŒì–´ ë‚´ê² ë‹¤. ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="3-proposed-method">3. Proposed Method</h2>
<h3 id="-block-residual-module-">[ Block Residual Module ]</h3>
<p>ìš°ì„ , BRMì€ HR informationì˜ ì¼ë¶€ë¥¼ resotoreí•˜ë©´ì„œ, high-level-layerë¡œ ì´ë¥¼ ì „ë‹¬í•˜ì—¬ restoreí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì ì—ì„œ BRMì€ super-resolution ë„ í•˜ë©´ì„œ, back-projectionë„ í•œê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br />
super resolution flow ëŠ” three convolutional layer ì™€ deconvolutional layerë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ë•Œ, outputì„ Ox ë¼ê³  í•˜ë©°, xëŠ” BRM indexì…ë‹ˆë‹¤. 
deconvolution / sub-pixel convolutional layer ì¤‘ deconvolutionì„ ì„ íƒí•œ ì´ìœ ëŠ” ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„± ì‚¬ì´ì—ì„œ ì ˆì¶©í•´ì„œ deconvolutionì„ ì„ íƒí–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì•„ë§ˆ, ì„±ëŠ¥ì„ ê³ ë ¤í–ˆë‹¤ë©´ sub-pixel convolutional layerë¥¼ ì‚¬ìš©í–ˆì„ ê²ƒìœ¼ë¡œ ìƒê° ë©ë‹ˆë‹¤. 
ë‹¤ìŒì˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ BRMì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  layer ëŠ”(3x3x64)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ down-sampling ì„ ì œì™¸í•˜ê³ ëŠ” stride=1,padding=1ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. local residual learningì€ convergence speed ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•¨ì´ë¼ê³  í•©ë‹ˆë‹¤.<br />
<img src="../assets/img/EBRN/EBRN_03.png" alt="Alt text" /><br /></p>

<h3 id="-embedded-block-residual-network-">[ Embedded Block Residual Network ]</h3>

<p><img src="../assets/img/EBRN.png" alt="Alt text" /><br />
<strong>Feature Extraction:</strong> conv1(3,256),conv2(3,64),conv3(3,64)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.<br />
<strong>Embedded Block Residual Learning:</strong> BRMì€ stack ë°©ì‹ì´ ì•„ë‹Œ, Embedding ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ëœë‹¤ê³  í•©ë‹ˆë‹¤.  <br />
<strong>Reconstruction:</strong> BRMì˜ outputì„ concatí•œ ë’¤, conv ë¥¼ í†µí•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.  <br /></p>

<p>ë•Œë¬¸ì—, low-frequency informationì€ BRM indexê°€ ë‚®ì€ ê²½ìš°ì—ì„œ ì „ë‹¬ëœë‹¤ê³  í•©ë‹ˆë‹¤. ì´ë¡œì¸í•´, ê¸°ì¡´ì˜ ë„¤íŠ¸ì›Œí¬ë“¤ì—ì„œ ê°„ë‹¨í•œ skip connectionìœ¼ë¡œ í’€ì—ˆë˜ ê²ƒì„ ì¡°ê¸ˆë” ì •êµí•˜ê²Œ ê°€ì ¸ê°ìœ¼ë¡œì¨, low-frequency over-fitingì„ ë§‰ì„ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.
ë°˜ë©´ì—, ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ê°€ ì˜ í’€ì–´ëƒˆë˜ high-frequency informationì€ ê¹Šì€ BRMì—ì„œ resotreí•œë‹¤ê³  í•©ë‹ˆë‹¤. ë•Œë¬¸ì— under-fiting issue ë„ í•´ê²°í•œë‹¤ê³  í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ BRM ì—ì„œ high-level indexëŠ” low-level indexì—ì„œ resotreí•˜ì§€ ëª»í•œ informationì„ resotreí•˜ë ¤ê³  í•˜ë©°, ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ êµ¬ì¡°ë¡œ ì¸í•´ì„œ, low-frequency informationë„ ìƒì§€ ì•Šìœ¼ë©´ì„œ, high-frequency informationë˜í•œ ì˜ restore í•œë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í•œê°€ì§€ point ëŠ” certain frequency informationì„ subnetwork of proper complexity ì™€ ì—°ê´€ì‹œí‚¬ìˆ˜ ìˆë‹¤ëŠ” ì ì— ìˆë‹¤ê³  í•©ë‹ˆë‹¤.</p>

<p>ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” Embedded Block Residual Learning ì˜ ì¥ì ì€ 2ê°€ì§€ë¼ê³  í•©ë‹ˆë‹¤.<br />
<strong>error propagation ë°©ì‹ì„ ë‹¨ì¶•í•˜ì—¬ converge spped ë¥¼ ë†’ì…ë‹ˆë‹¤.:</strong> <br />
<strong>intermediate feature map reuseë¥¼ íš¨ìœ¨ì ìœ¼ë¡œí•˜ì—¬ reconstructionì— ì´ì ì´ ìˆìŠµë‹ˆë‹¤.:</strong> <br />
ë¼ê³  ì €ëŠ” ì´í•´í•˜ì˜€ìŠµë‹ˆë‹¤.</p>

<h3 id="-loss-function-">[ Loss Function ]</h3>

<p>[DBPN] : https://github.com/thstkdgus35/EDSR-PyTorch â€œIncludes implementation of DBPNâ€</p>

:ET